{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e1fa6d-8b31-4605-a372-94745f1c8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad05b54-a930-4097-a480-ec1c81bc932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import os\n",
    "api_key = \"####\"\n",
    "import datetime as dt \n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "from newsapi import NewsApiClient\n",
    "import datetime as dt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a3c00e-29a4-4788-bc3d-edbba631653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1bd5e6c-3bc8-4106-ab1f-4eb5a0159756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16541\n"
     ]
    }
   ],
   "source": [
    "data = newsapi.get_everything(q='deepseek', from_param='2025-01-09')\n",
    "print(data['totalResults'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7b1e1d-24b7-4fe7-83ff-fbef236e7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cf7e5-950e-4ba5-b16c-fc5ded9a5129",
   "metadata": {},
   "source": [
    "## Deepseek Data Collection - Day One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac4f27b4-807f-49a2-aba0-a5471d5036d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>Steve Dent</td>\n",
       "      <td>China’s DeepSeek AI assistant becomes top free...</td>\n",
       "      <td>Chinese AI assistant DeepSeek has become the t...</td>\n",
       "      <td>https://consent.yahoo.com/v2/collectConsent?se...</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-01-27 13:44:45+00:00</td>\n",
       "      <td>If you click 'Accept all', we and our partners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>None</td>\n",
       "      <td>Is China's AI tool DeepSeek as good as it seems?</td>\n",
       "      <td>The artificial intelligence (AI) tool has shoc...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cx2jxvn0r51o</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_new...</td>\n",
       "      <td>2025-01-27 20:11:50+00:00</td>\n",
       "      <td>DeepSeek, a Chinese AI-chatbot app which launc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Gizmodo.com</td>\n",
       "      <td>Lucas Ropek</td>\n",
       "      <td>DeepSeek Releases Open-Source AI Image Generat...</td>\n",
       "      <td>Silicon Valley's Chinese competitor has releas...</td>\n",
       "      <td>https://gizmodo.com/deepseek-releases-open-sou...</td>\n",
       "      <td>https://gizmodo.com/app/uploads/2025/01/DeepSe...</td>\n",
       "      <td>2025-01-27 21:40:47+00:00</td>\n",
       "      <td>DeepSeek, the Chinese startup that has managed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Android Central</td>\n",
       "      <td>bradypsnyder@gmail.com (Brady Snyder)</td>\n",
       "      <td>Data Protection Day is a great day to stop usi...</td>\n",
       "      <td>DeepSeek's new AI models have taken the world ...</td>\n",
       "      <td>https://www.androidcentral.com/apps-software/d...</td>\n",
       "      <td>https://cdn.mos.cms.futurecdn.net/NYR57UzpFC38...</td>\n",
       "      <td>2025-01-28 17:41:36+00:00</td>\n",
       "      <td>Data Protection Day is today, Jan. 28, 2025, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Android Central</td>\n",
       "      <td>harish.jonnalagadda@futurenet.com (Harish Jonn...</td>\n",
       "      <td>DeepSeek already had a $1 trillion impact — an...</td>\n",
       "      <td>DeepSeek is showing the world that the latest ...</td>\n",
       "      <td>https://www.androidcentral.com/apps-software/d...</td>\n",
       "      <td>https://cdn.mos.cms.futurecdn.net/ceAoG3XT8se7...</td>\n",
       "      <td>2025-01-28 15:36:09+00:00</td>\n",
       "      <td>The tech world is collectively losing its mind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>None</td>\n",
       "      <td>Genbeta.com</td>\n",
       "      <td>Marcos Merino</td>\n",
       "      <td>Un jefe de OpenAI ve \"exagerada\" la repercusió...</td>\n",
       "      <td>La irrupción de DeepSeek y su modelo DeepSeek ...</td>\n",
       "      <td>https://www.genbeta.com/inteligencia-artificia...</td>\n",
       "      <td>https://i.blogs.es/683d41/9405b09a9b80579f281d...</td>\n",
       "      <td>2025-01-28 21:06:52+00:00</td>\n",
       "      <td>La irrupción de DeepSeek y su modelo DeepSeek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>None</td>\n",
       "      <td>Genbeta.com</td>\n",
       "      <td>José Alberto Lizana</td>\n",
       "      <td>DeepSeek: qué es, cómo funciona y todo lo que ...</td>\n",
       "      <td>El modelo de inteligencia artificial DeepSeek,...</td>\n",
       "      <td>https://www.genbeta.com/a-fondo/deepseek-que-c...</td>\n",
       "      <td>https://i.blogs.es/ea130b/deepseek-logo/840_56...</td>\n",
       "      <td>2025-01-27 13:11:40+00:00</td>\n",
       "      <td>El modelo de inteligencia artificial DeepSeek,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>None</td>\n",
       "      <td>heise online</td>\n",
       "      <td>Eva-Maria Weiß</td>\n",
       "      <td>Deepseek veröffentlicht multimodales Modell Ja...</td>\n",
       "      <td>Noch mehr Konkurrenz für das Silicon Valley: D...</td>\n",
       "      <td>https://www.heise.de/news/Deepseek-veroeffentl...</td>\n",
       "      <td>https://heise.cloudimg.io/bound/1200x1200/q85....</td>\n",
       "      <td>2025-01-28 07:06:00+00:00</td>\n",
       "      <td>Das chinesische KI-Unternehmen Deepseek, das g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>None</td>\n",
       "      <td>Genbeta.com</td>\n",
       "      <td>José Alberto Lizana</td>\n",
       "      <td>Enfrentamos a DeepSeek contra ChatGPT: sorpren...</td>\n",
       "      <td>Estamos viviendo un momento apabullante de la ...</td>\n",
       "      <td>https://www.genbeta.com/a-fondo/enfrentamos-a-...</td>\n",
       "      <td>https://i.blogs.es/87cd61/comparativa-/840_560...</td>\n",
       "      <td>2025-01-27 11:32:19+00:00</td>\n",
       "      <td>Estamos viviendo un momento apabullante de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>None</td>\n",
       "      <td>Genbeta.com</td>\n",
       "      <td>José Alberto Lizana</td>\n",
       "      <td>Si usas DeepSeek en su web o app, envías tus d...</td>\n",
       "      <td>El tema del momento en el campo tecnológico si...</td>\n",
       "      <td>https://www.genbeta.com/inteligencia-artificia...</td>\n",
       "      <td>https://i.blogs.es/312420/vitaly-gariev-vjq1fq...</td>\n",
       "      <td>2025-01-28 08:41:30+00:00</td>\n",
       "      <td>El tema del momento en el campo tecnológico si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id          source_name  \\\n",
       "0       None  Yahoo Entertainment   \n",
       "1       None             BBC News   \n",
       "2       None          Gizmodo.com   \n",
       "3       None      Android Central   \n",
       "4       None      Android Central   \n",
       "..       ...                  ...   \n",
       "94      None          Genbeta.com   \n",
       "95      None          Genbeta.com   \n",
       "96      None         heise online   \n",
       "97      None          Genbeta.com   \n",
       "98      None          Genbeta.com   \n",
       "\n",
       "                                               author  \\\n",
       "0                                          Steve Dent   \n",
       "1                                                None   \n",
       "2                                         Lucas Ropek   \n",
       "3               bradypsnyder@gmail.com (Brady Snyder)   \n",
       "4   harish.jonnalagadda@futurenet.com (Harish Jonn...   \n",
       "..                                                ...   \n",
       "94                                      Marcos Merino   \n",
       "95                                José Alberto Lizana   \n",
       "96                                     Eva-Maria Weiß   \n",
       "97                                José Alberto Lizana   \n",
       "98                                José Alberto Lizana   \n",
       "\n",
       "                                                title  \\\n",
       "0   China’s DeepSeek AI assistant becomes top free...   \n",
       "1    Is China's AI tool DeepSeek as good as it seems?   \n",
       "2   DeepSeek Releases Open-Source AI Image Generat...   \n",
       "3   Data Protection Day is a great day to stop usi...   \n",
       "4   DeepSeek already had a $1 trillion impact — an...   \n",
       "..                                                ...   \n",
       "94  Un jefe de OpenAI ve \"exagerada\" la repercusió...   \n",
       "95  DeepSeek: qué es, cómo funciona y todo lo que ...   \n",
       "96  Deepseek veröffentlicht multimodales Modell Ja...   \n",
       "97  Enfrentamos a DeepSeek contra ChatGPT: sorpren...   \n",
       "98  Si usas DeepSeek en su web o app, envías tus d...   \n",
       "\n",
       "                                          description  \\\n",
       "0   Chinese AI assistant DeepSeek has become the t...   \n",
       "1   The artificial intelligence (AI) tool has shoc...   \n",
       "2   Silicon Valley's Chinese competitor has releas...   \n",
       "3   DeepSeek's new AI models have taken the world ...   \n",
       "4   DeepSeek is showing the world that the latest ...   \n",
       "..                                                ...   \n",
       "94  La irrupción de DeepSeek y su modelo DeepSeek ...   \n",
       "95  El modelo de inteligencia artificial DeepSeek,...   \n",
       "96  Noch mehr Konkurrenz für das Silicon Valley: D...   \n",
       "97  Estamos viviendo un momento apabullante de la ...   \n",
       "98  El tema del momento en el campo tecnológico si...   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://consent.yahoo.com/v2/collectConsent?se...   \n",
       "1      https://www.bbc.com/news/articles/cx2jxvn0r51o   \n",
       "2   https://gizmodo.com/deepseek-releases-open-sou...   \n",
       "3   https://www.androidcentral.com/apps-software/d...   \n",
       "4   https://www.androidcentral.com/apps-software/d...   \n",
       "..                                                ...   \n",
       "94  https://www.genbeta.com/inteligencia-artificia...   \n",
       "95  https://www.genbeta.com/a-fondo/deepseek-que-c...   \n",
       "96  https://www.heise.de/news/Deepseek-veroeffentl...   \n",
       "97  https://www.genbeta.com/a-fondo/enfrentamos-a-...   \n",
       "98  https://www.genbeta.com/inteligencia-artificia...   \n",
       "\n",
       "                                           urlToImage  \\\n",
       "0                                                None   \n",
       "1   https://ichef.bbci.co.uk/news/1024/branded_new...   \n",
       "2   https://gizmodo.com/app/uploads/2025/01/DeepSe...   \n",
       "3   https://cdn.mos.cms.futurecdn.net/NYR57UzpFC38...   \n",
       "4   https://cdn.mos.cms.futurecdn.net/ceAoG3XT8se7...   \n",
       "..                                                ...   \n",
       "94  https://i.blogs.es/683d41/9405b09a9b80579f281d...   \n",
       "95  https://i.blogs.es/ea130b/deepseek-logo/840_56...   \n",
       "96  https://heise.cloudimg.io/bound/1200x1200/q85....   \n",
       "97  https://i.blogs.es/87cd61/comparativa-/840_560...   \n",
       "98  https://i.blogs.es/312420/vitaly-gariev-vjq1fq...   \n",
       "\n",
       "                 publishedAt  \\\n",
       "0  2025-01-27 13:44:45+00:00   \n",
       "1  2025-01-27 20:11:50+00:00   \n",
       "2  2025-01-27 21:40:47+00:00   \n",
       "3  2025-01-28 17:41:36+00:00   \n",
       "4  2025-01-28 15:36:09+00:00   \n",
       "..                       ...   \n",
       "94 2025-01-28 21:06:52+00:00   \n",
       "95 2025-01-27 13:11:40+00:00   \n",
       "96 2025-01-28 07:06:00+00:00   \n",
       "97 2025-01-27 11:32:19+00:00   \n",
       "98 2025-01-28 08:41:30+00:00   \n",
       "\n",
       "                                              content  \n",
       "0   If you click 'Accept all', we and our partners...  \n",
       "1   DeepSeek, a Chinese AI-chatbot app which launc...  \n",
       "2   DeepSeek, the Chinese startup that has managed...  \n",
       "3   Data Protection Day is today, Jan. 28, 2025, a...  \n",
       "4   The tech world is collectively losing its mind...  \n",
       "..                                                ...  \n",
       "94  La irrupción de DeepSeek y su modelo DeepSeek ...  \n",
       "95  El modelo de inteligencia artificial DeepSeek,...  \n",
       "96  Das chinesische KI-Unternehmen Deepseek, das g...  \n",
       "97  Estamos viviendo un momento apabullante de la ...  \n",
       "98  El tema del momento en el campo tecnológico si...  \n",
       "\n",
       "[99 rows x 9 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# References can be found from here --> https://www.youtube.com/watch?v=oUuIVJ96JsI\n",
    "\n",
    "def news_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert news API response data into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        news_data (dict): Dictionary containing news API response with 'articles' key\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing processed news articles data\n",
    "    \"\"\"\n",
    "    # Extract articles list\n",
    "    articles = data.get('articles', [])\n",
    "    \n",
    "    # Process each article to flatten the structure\n",
    "    processed_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source_id': article['source'].get('id'),\n",
    "            'source_name': article['source'].get('name'),\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'url': article.get('url'),\n",
    "            'urlToImage': article.get('urlToImage'),\n",
    "            'publishedAt': pd.to_datetime(article.get('publishedAt')),\n",
    "            'content': article.get('content')\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = news_to_dataframe(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3208cbb-8e86-45c5-b4db-d62fb747a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('deepseek_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36f9d5-67f8-4382-942f-51aeadff3e82",
   "metadata": {},
   "source": [
    "## Deepseek Data Collection - Day Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f0ef03-e6ce-472b-bd07-4edbc95b8a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           source_id          source_name  \\\n",
      "0               None  Yahoo Entertainment   \n",
      "1               None          Gizmodo.com   \n",
      "2               None            MacRumors   \n",
      "3   business-insider     Business Insider   \n",
      "4   business-insider     Business Insider   \n",
      "..               ...                  ...   \n",
      "94              None         Quartz India   \n",
      "95              None         Caschys Blog   \n",
      "96              None            Frandroid   \n",
      "97              None       Les Numériques   \n",
      "98              None            Frandroid   \n",
      "\n",
      "                                     author  \\\n",
      "0                             Mariella Moon   \n",
      "1                             Todd Feathers   \n",
      "2                              Tim Hardwick   \n",
      "3                            Ashley Stewart   \n",
      "4   tspirlet@insider.com (Thibault Spirlet)   \n",
      "..                                      ...   \n",
      "94                                Bruce Gil   \n",
      "95                                     Olli   \n",
      "96                          Maxence Glineur   \n",
      "97                             Benoit Bayle   \n",
      "98                           William Zimmer   \n",
      "\n",
      "                                                title  \\\n",
      "0   China's DeepSeek AI hit by information request...   \n",
      "1           The Knives Are Coming Out for DeepSeek AI   \n",
      "2   OpenAI Alleges DeepSeek Used Its Models for AI...   \n",
      "3   Microsoft CFO tells employees in an internal m...   \n",
      "4   Tech stocks stage partial recovery after marke...   \n",
      "..                                                ...   \n",
      "94  Microsoft thinks China's DeepSeek might have u...   \n",
      "95  Microsoft ermöglicht lokales Ausführen von Dee...   \n",
      "96  DeepSeek : on a testé la censure pratiquée par...   \n",
      "97  Actualité : Les concurrents chinois d'OpenAI a...   \n",
      "98  DeepSeek n’échappe pas à la censure : le chatb...   \n",
      "\n",
      "                                          description  \\\n",
      "0   China's DeepSeek AI has already caught the eye...   \n",
      "1   After the Chinese AI company's breakthrough, c...   \n",
      "2   OpenAI says it has uncovered evidence that Chi...   \n",
      "3   Microsoft CFO Amy Hood had a message for Micro...   \n",
      "4   Nvidia closed higher on Tuesday as some invest...   \n",
      "..                                                ...   \n",
      "94  Microsoft (MSFT) and OpenAI are investigating ...   \n",
      "95  Das Thema KI ist allgegenwärtig und Deepseek h...   \n",
      "96  DeepSeek fait beaucoup parler d'elle depuis pl...   \n",
      "97  La concurrence chinoise dans le domaine de l'i...   \n",
      "98  Alors que DeepSeek s’est récemment propulsé en...   \n",
      "\n",
      "                                                  url  \\\n",
      "0   https://consent.yahoo.com/v2/collectConsent?se...   \n",
      "1   https://gizmodo.com/the-knives-are-coming-out-...   \n",
      "2   https://www.macrumors.com/2025/01/29/openai-sa...   \n",
      "3   https://www.businessinsider.com/microsoft-cfo-...   \n",
      "4   https://markets.businessinsider.com/news/stock...   \n",
      "..                                                ...   \n",
      "94  https://qz.com/microsoft-openai-deepseek-chatg...   \n",
      "95  https://stadt-bremerhaven.de/microsoft-ermoegl...   \n",
      "96  https://www.frandroid.com/culture-tech/intelli...   \n",
      "97  https://www.lesnumeriques.com/intelligence-art...   \n",
      "98  https://www.frandroid.com/culture-tech/intelli...   \n",
      "\n",
      "                                           urlToImage  \\\n",
      "0                                                None   \n",
      "1   https://gizmodo.com/app/uploads/2025/01/DeepSe...   \n",
      "2   https://images.macrumors.com/t/q9T2pvMgTze4jrQ...   \n",
      "3   https://i.insider.com/679aa25e196626c40985b8d5...   \n",
      "4   https://i.insider.com/679a00a0eb4be2fff9a2b38c...   \n",
      "..                                                ...   \n",
      "94  https://i.kinja-img.com/image/upload/c_fill,h_...   \n",
      "95  https://stadt-bremerhaven.de/wp-content/upload...   \n",
      "96  https://c0.lestechnophiles.com/images.frandroi...   \n",
      "97  https://cdn.lesnumeriques.com/optim/news/23/23...   \n",
      "98  https://c0.lestechnophiles.com/images.frandroi...   \n",
      "\n",
      "                 publishedAt  \\\n",
      "0  2025-01-29 13:30:25+00:00   \n",
      "1  2025-01-29 19:00:15+00:00   \n",
      "2  2025-01-29 12:28:42+00:00   \n",
      "3  2025-01-29 22:17:12+00:00   \n",
      "4  2025-01-29 11:30:00+00:00   \n",
      "..                       ...   \n",
      "94 2025-01-29 13:16:00+00:00   \n",
      "95 2025-01-30 07:30:30+00:00   \n",
      "96 2025-01-29 18:14:38+00:00   \n",
      "97 2025-01-30 06:41:00+00:00   \n",
      "98 2025-01-29 16:40:25+00:00   \n",
      "\n",
      "                                              content  \n",
      "0   If you click 'Accept all', we and our partners...  \n",
      "1   When DeepSeek burst onto the scene this year t...  \n",
      "2   OpenAI says it has uncovered evidence that Chi...  \n",
      "3   Microsoft CFO Amy Hood.Stephen Brashear/Getty ...  \n",
      "4   AI-related stocks staged a partial recovery fo...  \n",
      "..                                                ...  \n",
      "94  In This Story\\r\\nMicrosoft (MSFT+2.86%\\r\\n) an...  \n",
      "95  Das Thema KI ist allgegenwärtig und Deepseek h...  \n",
      "96  DeepSeek fait beaucoup parler d’elle depuis pl...  \n",
      "97  Les concurrents chinois d'OpenAi accusés \"d'ex...  \n",
      "98  Alors que DeepSeek s’est récemment propulsé en...  \n",
      "\n",
      "[99 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Resources can be found from here --> ChatGPT was utilized for this section.\n",
    "\n",
    "newsapi = NewsApiClient(api_key=\"####\")  \n",
    "\n",
    "downloaded_file = 'deepseek_df.csv'\n",
    "\n",
    "# Function to check and load previously downloaded articles (URLs)\n",
    "def load_downloaded_articles(file_name: str = downloaded_file) -> set:\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "        return set(df['url'])  \n",
    "    else:\n",
    "        return set()  \n",
    "\n",
    "# Function to store new articles' URLs in a CSV\n",
    "def store_downloaded_articles(new_urls: list, file_name: str = downloaded_file):\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['url'])\n",
    "    \n",
    "    # Add new URLs to the DataFrame and save\n",
    "    new_df = pd.DataFrame({'url': new_urls})\n",
    "    df = pd.concat([df, new_df]).drop_duplicates(subset=['url'], keep='last')\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to convert the NewsAPI data to a DataFrame\n",
    "def news_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    articles = data.get('articles', [])\n",
    "    processed_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source_id': article['source'].get('id'),\n",
    "            'source_name': article['source'].get('name'),\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'url': article.get('url'),\n",
    "            'urlToImage': article.get('urlToImage'),\n",
    "            'publishedAt': pd.to_datetime(article.get('publishedAt')),\n",
    "            'content': article.get('content')\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    return df\n",
    "\n",
    "# Function to fetch and check if there are new articles\n",
    "def fetch_news(query: str, from_date: str) -> pd.DataFrame:\n",
    "    # Check the total number of results first to determine if there's new data\n",
    "    data = newsapi.get_everything(q=query, from_param=from_date)\n",
    "    total_results = data['totalResults']\n",
    "    \n",
    "    # If no new articles or less than 1,000 articles, return early\n",
    "    if total_results == 0:\n",
    "        print(\"No new articles available.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "    \n",
    "    downloaded_urls = load_downloaded_articles()\n",
    "    new_articles = [article for article in data['articles'] if article['url'] not in downloaded_urls]\n",
    "    \n",
    "    # If no new articles, return empty DataFrame\n",
    "    if len(new_articles) == 0:\n",
    "        print(\"No new articles to download.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert new articles to DataFrame\n",
    "    new_df = news_to_dataframe({'articles': new_articles})\n",
    "    \n",
    "    # Save URLs of new articles to avoid duplicates\n",
    "    new_urls = [article['url'] for article in new_articles]\n",
    "    store_downloaded_articles(new_urls)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "query = 'deepseek'\n",
    "from_date = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')  # Get articles from yesterday\n",
    "new_data = fetch_news(query, from_date)\n",
    "\n",
    "# Show and save the new data if available\n",
    "if not new_data.empty:\n",
    "    print(new_data)\n",
    "    new_data.to_csv('Deepseek_new_article.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cde25c-b67a-4bc4-996c-b0e1c87642f0",
   "metadata": {},
   "source": [
    "## Deepseek Data Collection - Day Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ec35242-a322-49f3-a7d5-8389e6ef2ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           source_id          source_name                 author  \\\n",
      "0   business-insider     Business Insider             Jane Zhang   \n",
      "1               None           Xataka.com        Yúbal Fernández   \n",
      "2   business-insider     Business Insider            Tim Paradis   \n",
      "3               None          Genbeta.com  Eva Rodriguez de Luis   \n",
      "4               None         Slashdot.org            EditorDavid   \n",
      "..               ...                  ...                    ...   \n",
      "81              None         Weeraman.com      Anuradha Weeraman   \n",
      "82       abc-news-au        ABC News (AU)            Alan Kohler   \n",
      "83              None      Overclockers.ru          Алексей Сычёв   \n",
      "84              None      Overclockers.ru              Madarator   \n",
      "85              None  Yahoo Entertainment                   None   \n",
      "\n",
      "                                                title  \\\n",
      "0   I'm an AI startup founder. Here are the 3 reas...   \n",
      "1   Guía DeepSeek: 36 funciones y cosas que puedes...   \n",
      "2                  Cheap AI could be good for workers   \n",
      "3   DeepSeek en local era justo lo que buscaba: un...   \n",
      "4   OpenAI Makes Surprise Livestream Today for 'De...   \n",
      "..                                                ...   \n",
      "81  Anuradha Weeraman: DeepSeek-R1, at the cusp of...   \n",
      "82  How DeepSeek changed the future of AI — and wh...   \n",
      "83  Акции TSMC упали на 6,6% после возвращения инв...   \n",
      "84  Власти Тайваня запретили всем госучреждениям и...   \n",
      "85  Alibaba named most admired Chinese internet fi...   \n",
      "\n",
      "                                          description  \\\n",
      "0   Steve Hsu, founder of AI startup SuperFocus, s...   \n",
      "1   Te traemos una guía en la que repasamos todo l...   \n",
      "2   Low-cost AI could let more workers access the ...   \n",
      "3   Cuando pensaba que entre ChatGPT y Microsoft C...   \n",
      "4   Just three hours ago, OpenAI made a surprise a...   \n",
      "..                                                ...   \n",
      "81  DeepSeek R1, the new entrant to the Large Lang...   \n",
      "82  What no-one talks about as the AI revolution u...   \n",
      "83                Тайвань всю прошлую неделю отдыхал.   \n",
      "84  Несмотря на все утверждения, что нейросети не ...   \n",
      "85                                               None   \n",
      "\n",
      "                                                  url  \\\n",
      "0   https://www.businessinsider.com/ai-startup-fou...   \n",
      "1   https://www.xataka.com/basics/guia-deepseek-36...   \n",
      "2   https://www.businessinsider.com/lower-cost-ai-...   \n",
      "3   https://www.genbeta.com/a-fondo/deepseek-local...   \n",
      "4   https://slashdot.org/story/25/02/02/2342245/op...   \n",
      "..                                                ...   \n",
      "81  https://weeraman.com/deepseek-r1-at-the-cusp-o...   \n",
      "82  https://www.abc.net.au/news/2025-02-03/deepsee...   \n",
      "83  https://overclockers.ru/hardnews/show/139320/a...   \n",
      "84  https://overclockers.ru/blog/ProKino/show/2062...   \n",
      "85  https://consent.yahoo.com/v2/collectConsent?se...   \n",
      "\n",
      "                                           urlToImage  \\\n",
      "0   https://i.insider.com/679d34ab7bb3f854015b6381...   \n",
      "1     https://i.blogs.es/000a37/deepseek/840_560.jpeg   \n",
      "2   https://i.insider.com/679d3b737bb3f854015b657d...   \n",
      "3      https://i.blogs.es/f480b4/deeeeep/840_560.jpeg   \n",
      "4              https://a.fsdn.com/sd/topics/ai_64.png   \n",
      "..                                                ...   \n",
      "81  https://images.unsplash.com/photo-159426723851...   \n",
      "82  https://live-production.wcms.abc-cdn.net.au/c1...   \n",
      "83  https://overclockers.ru/st/images/preview/L6mV...   \n",
      "84  https://overclockers.ru/st/legacy/blog/415476/...   \n",
      "85                                               None   \n",
      "\n",
      "                 publishedAt  \\\n",
      "0  2025-02-02 08:45:01+00:00   \n",
      "1  2025-02-02 13:01:58+00:00   \n",
      "2  2025-02-02 09:23:02+00:00   \n",
      "3  2025-02-02 17:01:57+00:00   \n",
      "4  2025-02-02 23:44:00+00:00   \n",
      "..                       ...   \n",
      "81 2025-02-02 14:37:44+00:00   \n",
      "82 2025-02-02 19:03:14+00:00   \n",
      "83 2025-02-03 01:38:00+00:00   \n",
      "84 2025-02-02 04:20:27+00:00   \n",
      "85 2025-02-02 09:30:00+00:00   \n",
      "\n",
      "                                              content  \n",
      "0   Steve Hsu (not pictured) says his AI startup, ...  \n",
      "1   Te traemos una guía en la que repasamos todo l...  \n",
      "2   As costs fall for artificial intelligence tool...  \n",
      "3   Cuando pensaba que entre ChatGPT y Microsoft C...  \n",
      "4   Just three hours ago, OpenAI made a surprise a...  \n",
      "..                                                ...  \n",
      "81  DeepSeek R1, the new entrant to the Large Lang...  \n",
      "82  Watching the global frenzy about the new Chine...  \n",
      "83  , Bloomberg, DeepSeek , . TSMC 6,6%, Foxconn 9...  \n",
      "84  , , , . DeepSeek. «» Bloomberg. \\r\\n    , . , ...  \n",
      "85  If you click 'Accept all', we and our partners...  \n",
      "\n",
      "[86 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "newsapi = NewsApiClient(api_key=\"####\")\n",
    "\n",
    "# Define the two downloaded files\n",
    "downloaded_file_1 = 'Deepseek_Day_One.csv'\n",
    "downloaded_file_2 = 'Deepseek_Day_Two.csv'\n",
    "\n",
    "# Function to load previously downloaded article URLs from multiple files\n",
    "def load_downloaded_articles(file_names: List[str]) -> set:\n",
    "    urls = set()\n",
    "    for file_name in file_names:\n",
    "        if os.path.exists(file_name):\n",
    "            df = pd.read_csv(file_name)\n",
    "            urls.update(set(df['url']))\n",
    "    return urls\n",
    "\n",
    "# Function to store new article URLs in a CSV file\n",
    "def store_downloaded_articles(new_urls: List[str], file_name: str):\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['url'])\n",
    "    \n",
    "    new_df = pd.DataFrame({'url': new_urls})\n",
    "    df = pd.concat([df, new_df]).drop_duplicates(subset=['url'], keep='last')\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to convert NewsAPI data to a DataFrame\n",
    "def news_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    articles = data.get('articles', [])\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source_id': article['source'].get('id'),\n",
    "            'source_name': article['source'].get('name'),\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'url': article.get('url'),\n",
    "            'urlToImage': article.get('urlToImage'),\n",
    "            'publishedAt': pd.to_datetime(article.get('publishedAt')),\n",
    "            'content': article.get('content')\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    return df\n",
    "\n",
    "# Function to fetch new articles based on a query and date,\n",
    "# while filtering out articles that have already been downloaded.\n",
    "def fetch_news(query: str, from_date: str) -> pd.DataFrame:\n",
    "    # Fetch data from NewsAPI\n",
    "    data = newsapi.get_everything(q=query, from_param=from_date)\n",
    "    total_results = data['totalResults']\n",
    "    \n",
    "    # If there are no results, return early.\n",
    "    if total_results == 0:\n",
    "        print(\"No new articles available.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    \n",
    "    # Load URLs from both downloaded files.\n",
    "    downloaded_urls = load_downloaded_articles([downloaded_file_1, downloaded_file_2])\n",
    "    # Filter for new articles (by URL)\n",
    "    new_articles = [article for article in data['articles'] if article['url'] not in downloaded_urls]\n",
    "    \n",
    "    # If there are no new articles, return early.\n",
    "    if not new_articles:\n",
    "        print(\"No new articles to download.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert the new articles to a DataFrame.\n",
    "    new_df = news_to_dataframe({'articles': new_articles})\n",
    "    \n",
    "    # Save the new article URLs.\n",
    "    new_urls = [article['url'] for article in new_articles]\n",
    "    # Here we choose to update one of the files (e.g., downloaded_file_2). Adjust as needed.\n",
    "    store_downloaded_articles(new_urls, downloaded_file_2)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "query = 'deepseek'\n",
    "# Get articles from yesterday\n",
    "from_date = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "new_data = fetch_news(query, from_date)\n",
    "\n",
    "# Show and save the new data if available\n",
    "if not new_data.empty:\n",
    "    print(new_data)\n",
    "    new_data.to_csv('Deepseek_Day_Three.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfeb7b-ab5f-421c-979b-43564f1def09",
   "metadata": {},
   "source": [
    "## Deepseek Data Collection - Day Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfeabe60-430c-40e0-b8d0-b5bf36045f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           source_id          source_name                            author  \\\n",
      "0   business-insider     Business Insider                   Dan DeFrancesco   \n",
      "1               None             Hackaday                       Donald Papp   \n",
      "2               None          Gizmodo.com                     Todd Feathers   \n",
      "3               None           Xataka.com                 Juan Carlos López   \n",
      "4               None       The New Yorker                      John Cassidy   \n",
      "..               ...                  ...                               ...   \n",
      "90              None               Futura  Edward Back, Journaliste hi-tech   \n",
      "91              None  Olhardigital.com.br           Leandro Costa Criscuolo   \n",
      "92              None         1pezeshk.com                      علیرضا مجیدی   \n",
      "93              None             TechSpot                       Rob Thubron   \n",
      "94              None              Sapo.pt                    Ana Sofia Neto   \n",
      "\n",
      "                                                title  \\\n",
      "0   What investors can learn from the DeepSeek tec...   \n",
      "1          More Details On Why DeepSeek is a Big Deal   \n",
      "2   SoftBank Is Betting Billions That OpenAI’s Age...   \n",
      "3   Este es uno de los secretos del éxito de DeepS...   \n",
      "4                 Is DeepSeek China’s Sputnik Moment?   \n",
      "..                                                ...   \n",
      "90  DeepSeek a-t-elle menti ? Des soupçons de tric...   \n",
      "91  Sam Altman alerta: OpenAI estaria no “lado err...   \n",
      "92  تجربه اجرای DeepSeek به‌صورت محلی روی لپ‌تاپ: ...   \n",
      "93  DeepSeek's AI costs far exceed $5.5 million cl...   \n",
      "94  DeepSeek ainda é muito vulnerável: modelo R1 f...   \n",
      "\n",
      "                                          description  \\\n",
      "0   The DeepSeek saga offers investors valuable le...   \n",
      "1   The DeepSeek large language models (LLM) have ...   \n",
      "2   It's another big swing from an investment firm...   \n",
      "3   El hardware utilizado por DeepSeek para entren...   \n",
      "4   The Chinese company’s low-cost, high-performan...   \n",
      "..                                                ...   \n",
      "90  Inquiets de voir la montée des intelligences a...   \n",
      "91  CEO da OpenAI opinou que a postura da empresa ...   \n",
      "92  تصور کن که همیشه یک چت‌بات هوش مصنوعی همراهت ب...   \n",
      "93  The claim that DeepSeek was able to train R1 u...   \n",
      "94  A DeepSeek está ainda a desbravar o mercado, c...   \n",
      "\n",
      "                                                  url  \\\n",
      "0   https://www.businessinsider.com/lessons-deepse...   \n",
      "1   https://hackaday.com/2025/02/03/more-details-o...   \n",
      "2   https://gizmodo.com/softbank-is-betting-billio...   \n",
      "3   https://www.xataka.com/robotica-e-ia/este-uno-...   \n",
      "4   https://www.newyorker.com/news/the-financial-p...   \n",
      "..                                                ...   \n",
      "90  https://www.futura-sciences.com/tech/actualite...   \n",
      "91  https://olhardigital.com.br/2025/02/03/pro/sam...   \n",
      "92  https://www.1pezeshk.com/archives/2025/02/i-tr...   \n",
      "93  https://www.techspot.com/news/106612-deepseek-...   \n",
      "94  https://pplware.sapo.pt/inteligencia-artificia...   \n",
      "\n",
      "                                           urlToImage  \\\n",
      "0   https://i.insider.com/679de88d7bb3f854015b6d0d...   \n",
      "1   https://hackaday.com/wp-content/uploads/2022/0...   \n",
      "2   https://gizmodo.com/app/uploads/2025/02/softba...   \n",
      "3   https://i.blogs.es/807891/deepseek-ap/840_560....   \n",
      "4   https://media.newyorker.com/photos/679d146db8b...   \n",
      "..                                                ...   \n",
      "90  https://cdn.futura-sciences.com/buildsv6/image...   \n",
      "91  https://olhardigital.com.br/wp-content/uploads...   \n",
      "92  https://www.1pezeshk.com/wp-content/uploads/20...   \n",
      "93  https://www.techspot.com/images2/news/bigimage...   \n",
      "94  https://pplware.sapo.pt/wp-content/uploads/202...   \n",
      "\n",
      "                 publishedAt  \\\n",
      "0  2025-02-03 12:23:17+00:00   \n",
      "1  2025-02-04 00:00:00+00:00   \n",
      "2  2025-02-03 20:45:34+00:00   \n",
      "3  2025-02-03 13:30:24+00:00   \n",
      "4  2025-02-03 11:00:00+00:00   \n",
      "..                       ...   \n",
      "90 2025-02-03 17:02:59+00:00   \n",
      "91 2025-02-03 22:13:00+00:00   \n",
      "92 2025-02-03 05:10:26+00:00   \n",
      "93 2025-02-03 10:19:00+00:00   \n",
      "94 2025-02-03 22:30:42+00:00   \n",
      "\n",
      "                                              content  \n",
      "0   Jakub Porzycki/NurPhoto via Getty Images\\r\\n<u...  \n",
      "1   The DeepSeek large language models (LLM) have ...  \n",
      "2   In what could be SoftBank’s newest trick to ma...  \n",
      "3   El hardware utilizado por DeepSeek para entren...  \n",
      "4   Last week, shortly before the start of the Chi...  \n",
      "..                                                ...  \n",
      "90  L'entreprise chinoise DeepSeek a non seulement...  \n",
      "91  Sam Altman, CEO da OpenAI, afirmou na semana p...  \n",
      "92  . (LLM – Large Language Models) . DeepSeek-R1 ...  \n",
      "93  In brief: China's DeepSeek threw the multi-bil...  \n",
      "94  A DeepSeek está ainda a desbravar o mercado, c...  \n",
      "\n",
      "[95 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "newsapi = NewsApiClient(api_key=\"####\")\n",
    "\n",
    "# Define the two downloaded files\n",
    "downloaded_file_1 = 'Deepseek_Day_One.csv'\n",
    "downloaded_file_2 = 'Deepseek_Day_Two.csv'\n",
    "downloaded_file_3 = 'Deepseek_Day_Three.csv'\n",
    "\n",
    "# Function to load previously downloaded article URLs from multiple files\n",
    "def load_downloaded_articles(file_names: List[str]) -> set:\n",
    "    urls = set()\n",
    "    for file_name in file_names:\n",
    "        if os.path.exists(file_name):\n",
    "            df = pd.read_csv(file_name)\n",
    "            urls.update(set(df['url']))\n",
    "    return urls\n",
    "\n",
    "# Function to store new article URLs in a CSV file\n",
    "def store_downloaded_articles(new_urls: List[str], file_name: str):\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['url'])\n",
    "    \n",
    "    new_df = pd.DataFrame({'url': new_urls})\n",
    "    df = pd.concat([df, new_df]).drop_duplicates(subset=['url'], keep='last')\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to convert NewsAPI data to a DataFrame\n",
    "def news_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    articles = data.get('articles', [])\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source_id': article['source'].get('id'),\n",
    "            'source_name': article['source'].get('name'),\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'url': article.get('url'),\n",
    "            'urlToImage': article.get('urlToImage'),\n",
    "            'publishedAt': pd.to_datetime(article.get('publishedAt')),\n",
    "            'content': article.get('content')\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    return df\n",
    "\n",
    "# Function to fetch new articles based on a query and date,\n",
    "# while filtering out articles that have already been downloaded.\n",
    "def fetch_news(query: str, from_date: str) -> pd.DataFrame:\n",
    "    # Fetch data from NewsAPI\n",
    "    data = newsapi.get_everything(q=query, from_param=from_date)\n",
    "    total_results = data['totalResults']\n",
    "    \n",
    "    # If there are no results, return early.\n",
    "    if total_results == 0:\n",
    "        print(\"No new articles available.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    \n",
    "    # Load URLs from both downloaded files.\n",
    "    downloaded_urls = load_downloaded_articles([downloaded_file_1, downloaded_file_2])\n",
    "    # Filter for new articles (by URL)\n",
    "    new_articles = [article for article in data['articles'] if article['url'] not in downloaded_urls]\n",
    "    \n",
    "    # If there are no new articles, return early.\n",
    "    if not new_articles:\n",
    "        print(\"No new articles to download.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert the new articles to a DataFrame.\n",
    "    new_df = news_to_dataframe({'articles': new_articles})\n",
    "    \n",
    "    # Save the new article URLs.\n",
    "    new_urls = [article['url'] for article in new_articles]\n",
    "    # Here we choose to update one of the files (e.g., downloaded_file_2). Adjust as needed.\n",
    "    store_downloaded_articles(new_urls, downloaded_file_2)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "query = 'deepseek'\n",
    "# Get articles from yesterday\n",
    "from_date = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "new_data = fetch_news(query, from_date)\n",
    "\n",
    "# Show and save the new data if available\n",
    "if not new_data.empty:\n",
    "    print(new_data)\n",
    "    new_data.to_csv('Deepseek_Day_Four.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97942585-da54-4cbc-a8a1-b1dafa1ef692",
   "metadata": {},
   "source": [
    "## Deepseek Data Collection - Day Five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6dee56d-654f-4906-acc3-5cbede605de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new articles to download.\n"
     ]
    }
   ],
   "source": [
    "newsapi = NewsApiClient(api_key=\"####\")\n",
    "\n",
    "# Define the two downloaded files\n",
    "downloaded_file_1 = 'Deepseek_Day_One.csv'\n",
    "downloaded_file_2 = 'Deepseek_Day_Two.csv'\n",
    "downloaded_file_3 = 'Deepseek_Day_Three.csv'\n",
    "downloaded_file_4 = 'Deepseek_Day_Four.csv'\n",
    "\n",
    "# Function to load previously downloaded article URLs from multiple files\n",
    "def load_downloaded_articles(file_names: List[str]) -> set:\n",
    "    urls = set()\n",
    "    for file_name in file_names:\n",
    "        if os.path.exists(file_name):\n",
    "            df = pd.read_csv(file_name)\n",
    "            urls.update(set(df['url']))\n",
    "    return urls\n",
    "\n",
    "# Function to store new article URLs in a CSV file\n",
    "def store_downloaded_articles(new_urls: List[str], file_name: str):\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['url'])\n",
    "    \n",
    "    new_df = pd.DataFrame({'url': new_urls})\n",
    "    df = pd.concat([df, new_df]).drop_duplicates(subset=['url'], keep='last')\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to convert NewsAPI data to a DataFrame\n",
    "def news_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    articles = data.get('articles', [])\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source_id': article['source'].get('id'),\n",
    "            'source_name': article['source'].get('name'),\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'url': article.get('url'),\n",
    "            'urlToImage': article.get('urlToImage'),\n",
    "            'publishedAt': pd.to_datetime(article.get('publishedAt')),\n",
    "            'content': article.get('content')\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    return df\n",
    "\n",
    "# Function to fetch new articles based on a query and date,\n",
    "# while filtering out articles that have already been downloaded.\n",
    "def fetch_news(query: str, from_date: str) -> pd.DataFrame:\n",
    "    # Fetch data from NewsAPI\n",
    "    data = newsapi.get_everything(q=query, from_param=from_date)\n",
    "    total_results = data['totalResults']\n",
    "    \n",
    "    # If there are no results, return early.\n",
    "    if total_results == 0:\n",
    "        print(\"No new articles available.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    " # Load URLs from both downloaded files.\n",
    "    downloaded_urls = load_downloaded_articles([downloaded_file_1, downloaded_file_2])\n",
    "    # Filter for new articles (by URL)\n",
    "    new_articles = [article for article in data['articles'] if article['url'] not in downloaded_urls]\n",
    "    \n",
    "    # If there are no new articles, return early.\n",
    "    if not new_articles:\n",
    "        print(\"No new articles to download.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert the new articles to a DataFrame.\n",
    "    new_df = news_to_dataframe({'articles': new_articles})\n",
    "    \n",
    "    # Save the new article URLs.\n",
    "    new_urls = [article['url'] for article in new_articles]\n",
    "    # Here we choose to update one of the files (e.g., downloaded_file_2). Adjust as needed.\n",
    "    store_downloaded_articles(new_urls, downloaded_file_2)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "query = 'deepseek'\n",
    "# Get articles from yesterday\n",
    "from_date = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "new_data = fetch_news(query, from_date)\n",
    "\n",
    "# Show and save the new data if available\n",
    "if not new_data.empty:\n",
    "    print(new_data)\n",
    "    new_data.to_csv('Deepseek_Day_Six.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9fb876d-e464-46c2-b04c-f4a2c6608875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new articles to download.\n"
     ]
    }
   ],
   "source": [
    "newsapi = NewsApiClient(api_key=\"####\")\n",
    "\n",
    "# Define the two downloaded files\n",
    "downloaded_file_1 = 'Deepseek_Day_One.csv'\n",
    "downloaded_file_2 = 'Deepseek_Day_Two.csv'\n",
    "downloaded_file_3 = 'Deepseek_Day_Three.csv'\n",
    "downloaded_file_4 = 'Deepseek_Day_Four.csv'\n",
    "downloaded_file_5 = 'Deepseek_Day_Five.csv'\n",
    "\n",
    "# Function to load previously downloaded article URLs from multiple files\n",
    "def load_downloaded_articles(file_names: List[str]) -> set:\n",
    "    urls = set()\n",
    "    for file_name in file_names:\n",
    "        if os.path.exists(file_name):\n",
    "            df = pd.read_csv(file_name)\n",
    "            urls.update(set(df['url']))\n",
    "    return urls\n",
    "\n",
    "# Function to store new article URLs in a CSV file\n",
    "def store_downloaded_articles(new_urls: List[str], file_name: str):\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['url'])\n",
    "    \n",
    "    new_df = pd.DataFrame({'url': new_urls})\n",
    "    df = pd.concat([df, new_df]).drop_duplicates(subset=['url'], keep='last')\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Function to convert NewsAPI data to a DataFrame\n",
    "def news_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    articles = data.get('articles', [])\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source_id': article['source'].get('id'),\n",
    "            'source_name': article['source'].get('name'),\n",
    "            'author': article.get('author'),\n",
    "            'title': article.get('title'),\n",
    "            'description': article.get('description'),\n",
    "            'url': article.get('url'),\n",
    "            'urlToImage': article.get('urlToImage'),\n",
    "            'publishedAt': pd.to_datetime(article.get('publishedAt')),\n",
    "            'content': article.get('content')\n",
    "        }\n",
    "        processed_articles.append(processed_article)\n",
    "    \n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    return df\n",
    "\n",
    "# Function to fetch new articles based on a query and date,\n",
    "# while filtering out articles that have already been downloaded.\n",
    "def fetch_news(query: str, from_date: str) -> pd.DataFrame:\n",
    "    # Fetch data from NewsAPI\n",
    "    data = newsapi.get_everything(q=query, from_param=from_date)\n",
    "    total_results = data['totalResults']\n",
    "    \n",
    "    # If there are no results, return early.\n",
    "    if total_results == 0:\n",
    "        print(\"No new articles available.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "\n",
    " # Load URLs from both downloaded files.\n",
    "    downloaded_urls = load_downloaded_articles([downloaded_file_1, downloaded_file_2])\n",
    "    # Filter for new articles (by URL)\n",
    "    new_articles = [article for article in data['articles'] if article['url'] not in downloaded_urls]\n",
    "    \n",
    "    # If there are no new articles, return early.\n",
    "    if not new_articles:\n",
    "        print(\"No new articles to download.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert the new articles to a DataFrame.\n",
    "    new_df = news_to_dataframe({'articles': new_articles})\n",
    "    \n",
    "    # Save the new article URLs.\n",
    "    new_urls = [article['url'] for article in new_articles]\n",
    "    # Here we choose to update one of the files (e.g., downloaded_file_2). Adjust as needed.\n",
    "    store_downloaded_articles(new_urls, downloaded_file_2)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "query = 'deepseek'\n",
    "# Get articles from yesterday\n",
    "from_date = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "new_data = fetch_news(query, from_date)\n",
    "\n",
    "# Show and save the new data if available\n",
    "if not new_data.empty:\n",
    "    print(new_data)\n",
    "    new_data.to_csv('Deepseek_Day_Six.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6daaafa-6ca2-44e5-acb3-3e4c16e7a283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
