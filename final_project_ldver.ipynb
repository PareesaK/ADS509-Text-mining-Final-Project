{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222359ad-938e-4562-b344-74a880b1e77e",
   "metadata": {},
   "source": [
    "# ADS 509 Text Mining Project\n",
    "\n",
    "**Lorena Dorado & Parisa Kamizi** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e633187-7547-4b65-9f38-678770f04ca1",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ef2292-8fdc-474f-9c62-358d9eab224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from langdetect import DetectorFactory\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Topic Modeling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c267811-b3e8-4b4a-99c0-13b0b80de743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 14:33:38,853 - INFO - Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('text_mining.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6195ad-9e5f-4ab2-b72d-92544e785417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up text processing configurations\n",
    "# stop_words = set(stopwords.words('english')) ADD other languages\n",
    "stop_words = set(stopwords.words('english') + \n",
    "                 stopwords.words('spanish') + \n",
    "                 stopwords.words('french') + \n",
    "                 stopwords.words('german'))\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = {'ul', 'li', 'ol', 'div', 'span', 'href', 'src', 'img', 'p', 'br', 'nbsp', 'char', 'id', 'av', 'lv'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "website_stopwords = ['transparency', 'accept', 'partner', 'click', 'consent', 'cookie', 'policy', 'privacy', 'terms', 'use', 'agreement', 'site']\n",
    "stop_words.update(website_stopwords)\n",
    "\n",
    "punctuation_set = set(string.punctuation) - {\"#\"}  # Keep hashtags\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b9603-9b4d-487e-8753-19d387fb763d",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3876a3a7-c344-4e66-9dc9-f6e4ccb37c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 14:33:38,886 - INFO - Successfully loaded .\\Deepseek_Day_Five.csv\n",
      "2025-02-22 14:33:38,899 - INFO - Successfully loaded .\\Deepseek_Day_Four.csv\n",
      "2025-02-22 14:33:38,899 - INFO - Successfully loaded .\\Deepseek_Day_One.csv\n",
      "2025-02-22 14:33:38,912 - INFO - Successfully loaded .\\Deepseek_Day_Three.csv\n",
      "2025-02-22 14:33:38,912 - INFO - Successfully loaded .\\Deepseek_Day_Two.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 755 entries, 0 to 754\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   source       570 non-null    object             \n",
      " 1   date         570 non-null    datetime64[ns, UTC]\n",
      " 2   text         570 non-null    object             \n",
      " 3   title        570 non-null    object             \n",
      " 4   description  552 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), object(4)\n",
      "memory usage: 29.6+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "             source                      date  \\\n",
      "0       Gizmodo.com 2025-02-04 15:00:56+00:00   \n",
      "1  Business Insider 2025-02-04 18:25:21+00:00   \n",
      "2      Substack.com 2025-02-04 21:28:04+00:00   \n",
      "3  Business Insider 2025-02-04 21:26:32+00:00   \n",
      "4      heise online 2025-02-04 14:00:00+00:00   \n",
      "\n",
      "                                                text  \\\n",
      "0  Usually when large language models are given t...   \n",
      "1  Meta's CTO, Andrew Bosworth, talked about Deep...   \n",
      "2  The release of DeepSeek has upset American tec...   \n",
      "3  Sundar Pichai speaks during a Google I/O confe...   \n",
      "4  Inhaltsverzeichnis\\r\\nDer japanische Technolog...   \n",
      "\n",
      "                                               title  \\\n",
      "0    DeepSeek Gets an ‘F’ in Safety From Researchers   \n",
      "1  Meta's CTO says he called the whole DeepSeek t...   \n",
      "2                      DeepSeek and Double Standards   \n",
      "3  Alphabet is planning to spend big again this y...   \n",
      "4  KI-Update kompakt: Softbank, DeepSeek, OpenAI,...   \n",
      "\n",
      "                                         description  \n",
      "0  The model failed to block a single attack atte...  \n",
      "1  Meta's CTO said the DeepSeek news cycle has be...  \n",
      "2  The release of DeepSeek has upset American tec...  \n",
      "3  Alphabet's fourth-quarter earnings showed slow...  \n",
      "4  Das \"KI-Update\" liefert werktäglich eine Zusam...  \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "column_mapping = {\n",
    "    'source_name': 'source',\n",
    "    'publishedAt': 'date',\n",
    "    'content': 'text',\n",
    "    'title': 'title',\n",
    "    'description': 'description'\n",
    "}\n",
    "\n",
    "csv_files = glob.glob(os.path.join('.', '*.csv'))\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file)\n",
    "        dfs.append(temp_df)\n",
    "        logger.info(f\"Successfully loaded {file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file {file}: {str(e)}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No CSV files were successfully loaded\")\n",
    "\n",
    "news_df = pd.concat(dfs, ignore_index=True)\n",
    "news_df = news_df[column_mapping.keys()].rename(columns=column_mapping)\n",
    "news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "\n",
    "print(\"\\nInitial Dataset Info:\")\n",
    "print(news_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28dfdc-358b-484e-8e96-ce90554fb9f4",
   "metadata": {},
   "source": [
    "## Data Cleaning with Tokenization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5cf8811-890e-4733-b356-1e0f2f9c2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Text processing pipeline functions\n",
    "def validate_text(text):\n",
    "    \"\"\"Check if text is valid\"\"\"\n",
    "    return \"\" if not isinstance(text, str) or pd.isna(text) else text\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    return (html.unescape(text)\n",
    "            .lower()\n",
    "            .replace('\\n', ' ')\n",
    "            .replace('\\r', ' '))\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"Remove special characters and patterns\"\"\"\n",
    "    return re.sub(r'\\[\\+\\d+ chars\\]|https?://\\S+|â€™|â€\"|[^\\w\\s\\-\\'.,!?]', ' ', text)\n",
    "\n",
    "def remove_num_patterns(text):\n",
    "    \"\"\"Remove date patterns and numbers (including those with commas)\"\"\"\n",
    "    return re.sub(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b|\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b|\\b\\d+\\b', '', text)\n",
    "\n",
    "def clean_whitespace(text):\n",
    "    \"\"\"Clean extra whitespace\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"Remove punctuation\"\"\"\n",
    "    return \"\".join(ch for ch in text if ch not in punctuation_set)\n",
    "\n",
    "def get_tokens(text):\n",
    "    \"\"\"Get tokens without stopwords, lemmatize, remove numbers, and filter short tokens\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc \n",
    "            if token.is_alpha \n",
    "            and len(token.text) > 2 \n",
    "            and not token.is_stop \n",
    "            and token.lemma_.lower() not in stop_words \n",
    "            and not token.like_num]\n",
    "\n",
    "def detect_lang(text):\n",
    "    \"\"\"Detect language safely\"\"\"\n",
    "    try:\n",
    "        return 'unknown' if len(text.strip()) < 50 else detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove duplicate articles based on content similarity\"\"\"\n",
    "    df['text_signature'] = df['title'] + df['text'].str[:200]\n",
    "    original_len = len(df)\n",
    "    df = df.drop_duplicates(subset=['text_signature'])\n",
    "    df = df.drop('text_signature', axis=1)\n",
    "    logger.info(f\"Removed {original_len - len(df)} duplicate articles\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c831ffce-1614-4801-a677-c5308505f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, steps=None):\n",
    "    if steps is None:\n",
    "        steps = ['validate', 'basic', 'special', 'num_patterns', 'whitespace', 'punct', 'tokens']\n",
    "    \n",
    "    text = validate_text(text)\n",
    "    \n",
    "    pipeline_steps = {\n",
    "        'validate': validate_text,\n",
    "        'basic': basic_clean,\n",
    "        'special': remove_special_chars,\n",
    "        'num_patterns': remove_num_patterns,\n",
    "        'whitespace': clean_whitespace,\n",
    "        'punct': remove_punct,\n",
    "        'tokens': get_tokens\n",
    "    }\n",
    "    \n",
    "    for step in steps:\n",
    "        text = pipeline_steps[step](text)\n",
    "    \n",
    "    return text if isinstance(text, list) else text  # Return tokens as a list if the last step was tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f2a4ff-c859-4361-ab3d-2d30322a9f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 14:33:38,977 - INFO - Removing duplicates...\n",
      "2025-02-22 14:33:38,977 - INFO - Removed 286 duplicate articles\n",
      "2025-02-22 14:33:38,977 - INFO - Processing text...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 469/469 [00:00<00:00, 24469.21it/s]\n",
      "2025-02-22 14:33:39,012 - INFO - Detecting languages...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 469/469 [00:03<00:00, 125.18it/s]\n",
      "2025-02-22 14:33:42,759 - INFO - Creating cleaned content...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 469/469 [00:00<00:00, 30240.25it/s]\n",
      "2025-02-22 14:33:42,790 - INFO - Generating tokens...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 469/469 [00:03<00:00, 117.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame columns:\n",
      "['source', 'date', 'text', 'title', 'description', 'clean_text', 'language', 'cleaned_content', 'Cleaned_Text', 'Tokens']\n",
      "\n",
      "Sample of processed data:\n",
      "             source                      date  \\\n",
      "0       Gizmodo.com 2025-02-04 15:00:56+00:00   \n",
      "1  Business Insider 2025-02-04 18:25:21+00:00   \n",
      "2      Substack.com 2025-02-04 21:28:04+00:00   \n",
      "3  Business Insider 2025-02-04 21:26:32+00:00   \n",
      "4      heise online 2025-02-04 14:00:00+00:00   \n",
      "\n",
      "                                          clean_text language  \\\n",
      "0  usually when large language models are given t...       en   \n",
      "1  meta's cto, andrew bosworth, talked about deep...       en   \n",
      "2  the release of deepseek has upset american tec...       en   \n",
      "3  sundar pichai speaks during a google i o confe...       en   \n",
      "4  inhaltsverzeichnis der japanische technologiek...       de   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [usually, large, language, model, give, test, ...  \n",
      "1  [metas, cto, andrew, bosworth, talk, deepseek,...  \n",
      "2  [release, deepseek, upset, american, tech, ceo...  \n",
      "3  [sundar, pichai, speak, google, conferencejust...  \n",
      "4  [inhaltsverzeichnis, japanische, technologieko...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "logger.info(\"Removing duplicates...\")\n",
    "news_df = remove_duplicates(news_df)\n",
    "\n",
    "# Process text\n",
    "logger.info(\"Processing text...\")\n",
    "tqdm.pandas()\n",
    "news_df['clean_text'] = news_df['text'].progress_apply(\n",
    "    lambda x: process_text(x, ['validate', 'basic', 'special', 'num_patterns', 'whitespace']))\n",
    "\n",
    "# Detect languages\n",
    "logger.info(\"Detecting languages...\")\n",
    "news_df['language'] = news_df['clean_text'].progress_apply(detect_lang)\n",
    "\n",
    "# Create cleaned content\n",
    "logger.info(\"Creating cleaned content...\")\n",
    "news_df['cleaned_content'] = news_df['text'].progress_apply(\n",
    "    lambda x: process_text(x, ['validate', 'basic', 'punct', 'whitespace']))\n",
    "\n",
    "# Generate tokens\n",
    "logger.info(\"Generating tokens...\")\n",
    "def get_cleaned_text_and_tokens(text):\n",
    "    cleaned = process_text(text, ['validate', 'basic', 'punct', 'whitespace'])\n",
    "    tokens = get_tokens(cleaned)\n",
    "    return ' '.join(tokens), tokens\n",
    "\n",
    "temp_results = news_df['text'].progress_apply(get_cleaned_text_and_tokens)\n",
    "news_df['Cleaned_Text'] = temp_results.apply(lambda x: x[0])\n",
    "news_df['Tokens'] = temp_results.apply(lambda x: x[1])\n",
    "\n",
    "# Display processed data sample\n",
    "print(\"\\nProcessed DataFrame columns:\")\n",
    "print(news_df.columns.tolist())\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(news_df[['source', 'date', 'clean_text', 'language', 'Tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea86121-29d9-400a-8125-db390416df01",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1070262e-5c02-482c-b99c-fea3b6e5dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics for 'News Articles':\n",
      "--------------------------------------------------\n",
      "Total documents: 469\n",
      "\n",
      "Token-level Statistics:\n",
      "Total tokens: 7,307\n",
      "Unique tokens: 3,493\n",
      "Total characters: 49,720\n",
      "Lexical diversity: 0.478\n",
      "\n",
      "Top 10 most frequent tokens:\n",
      "deepseek: 415\n",
      "openai: 132\n",
      "model: 77\n",
      "chatgpt: 62\n",
      "chinese: 58\n",
      "nvidia: 57\n",
      "artificial: 56\n",
      "startup: 44\n",
      "tech: 43\n",
      "china: 39\n"
     ]
    }
   ],
   "source": [
    "def descriptive_stats_all(df: pd.DataFrame, tokens_col: str = 'Tokens', \n",
    "                         text_col: str = 'text', title: str = \"Dataset\",\n",
    "                         num_tokens: int = 5, plot: bool = True) -> Dict[str, Any]:\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    stats = {}\n",
    "    \n",
    "    # Get all tokens\n",
    "    all_tokens = [token for tokens in df[tokens_col] for token in tokens]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Basic token statistics\n",
    "    stats['total_tokens'] = len(all_tokens)\n",
    "    stats['unique_tokens'] = len(set(all_tokens))\n",
    "    stats['total_characters'] = len(''.join(all_tokens))\n",
    "    stats['lexical_diversity'] = stats['unique_tokens'] / stats['total_tokens'] if stats['total_tokens'] > 0 else 0\n",
    "    \n",
    "    # Document statistics\n",
    "    stats['total_documents'] = len(df)\n",
    "    \n",
    "    # Token length statistics\n",
    "    token_lengths = [len(token) for token in all_tokens]\n",
    "\n",
    "    # Top tokens\n",
    "    stats['top_tokens'] = token_counts.most_common(num_tokens)\n",
    "    \n",
    "    # Print results if verbose\n",
    "    print(f\"\\nDescriptive Statistics for '{title}':\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total documents: {stats['total_documents']:,}\")\n",
    "    \n",
    "    print(f\"\\nToken-level Statistics:\")\n",
    "    print(f\"Total tokens: {stats['total_tokens']:,}\")\n",
    "    print(f\"Unique tokens: {stats['unique_tokens']:,}\")\n",
    "    print(f\"Total characters: {stats['total_characters']:,}\")\n",
    "    print(f\"Lexical diversity: {stats['lexical_diversity']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop {num_tokens} most frequent tokens:\")\n",
    "    for token, count in stats['top_tokens']:\n",
    "        print(f\"{token}: {count:,}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "stats = descriptive_stats_all(news_df, tokens_col='Tokens', text_col='text',\n",
    "                              title=\"News Articles\", num_tokens=10, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed838cc1-ea42-4851-8714-9121aa5c79c0",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd8de81-577d-4be1-a7b7-17d97af2e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=list(stop_words), max_features=5000, max_df=0.95, min_df=2)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news_df['clean_text'])\n",
    "\n",
    "# Create Count vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=list(stop_words), max_features=5000, max_df=0.95, min_df=2)\n",
    "count_matrix = count_vectorizer.fit_transform(news_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "556be751-af49-467f-9389-2dbf66c217ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display topics\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271b3a8-e155-42d3-89d6-d0ccdde8a312",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015a4a5c-39d0-4db2-ada9-8a47ac537c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF Topics:\n",
      "\n",
      "Topic 00\n",
      "  deepseek (23.87)\n",
      "  r1 (6.87)\n",
      "  chatgpt (3.60)\n",
      "  nvidia (2.62)\n",
      "  v3 (1.48)\n",
      "\n",
      "Topic 01\n",
      "  partners (9.61)\n",
      "  iab (9.61)\n",
      "  words (9.61)\n",
      "  information (9.49)\n",
      "  part (9.46)\n",
      "\n",
      "Topic 02\n",
      "  openai (12.64)\n",
      "  o3 (4.69)\n",
      "  mini (4.65)\n",
      "  altman (4.11)\n",
      "  chatgpt (4.09)\n",
      "\n",
      "Topic 03\n",
      "  chinese (2.21)\n",
      "  model (2.10)\n",
      "  tech (1.63)\n",
      "  new (1.46)\n",
      "  startup (1.31)\n",
      "\n",
      "Topic 04\n",
      "  artificial (4.38)\n",
      "  inteligencia (4.19)\n",
      "  ia (4.17)\n",
      "  modelo (3.01)\n",
      "  modelos (2.14)\n"
     ]
    }
   ],
   "source": [
    "# Fit NMF model\n",
    "n_topics = 5\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "nmf_output = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Display the topics\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"NMF Topics:\")\n",
    "display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e293b-af35-49ed-9dda-dd00f0d6d348",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a316060-8177-4ae6-8877-ce4c56b0c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSA Topics:\n",
      "\n",
      "Topic 00\n",
      "  deepseek (8.90)\n",
      "  r1 (2.93)\n",
      "  openai (2.63)\n",
      "  chatgpt (2.19)\n",
      "  nvidia (1.16)\n",
      "\n",
      "Topic 01\n",
      "  iab (10.82)\n",
      "  partners (10.82)\n",
      "  words (10.82)\n",
      "  information (10.67)\n",
      "  part (10.65)\n",
      "\n",
      "Topic 02\n",
      "  openai (11.14)\n",
      "  mini (4.91)\n",
      "  o3 (4.90)\n",
      "  altman (3.96)\n",
      "  chatgpt (3.14)\n",
      "\n",
      "Topic 03\n",
      "  chinese (2.46)\n",
      "  model (2.10)\n",
      "  tech (1.84)\n",
      "  new (1.60)\n",
      "  startup (1.47)\n",
      "\n",
      "Topic 04\n",
      "  ia (10.43)\n",
      "  artificial (10.32)\n",
      "  inteligencia (10.04)\n",
      "  modelo (7.05)\n",
      "  china (5.28)\n"
     ]
    }
   ],
   "source": [
    "# Fit LSA model\n",
    "lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "lsa_output = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(\"\\nLSA Topics:\")\n",
    "display_topics(lsa_model, tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c347b6-66e0-408a-97f7-0370e330c14f",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e830e45-0f4a-4823-b223-0de5362d0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Topics:\n",
      "\n",
      "Topic 00\n",
      "  deepseek (3.51)\n",
      "  di (1.95)\n",
      "  including (1.62)\n",
      "  week (1.60)\n",
      "  new (1.44)\n",
      "\n",
      "Topic 01\n",
      "  deepseek (11.19)\n",
      "  nvidia (3.39)\n",
      "  r1 (2.06)\n",
      "  artificial (1.48)\n",
      "  chatgpt (1.24)\n",
      "\n",
      "Topic 02\n",
      "  deepseek (7.96)\n",
      "  ia (2.27)\n",
      "  inteligencia (1.95)\n",
      "  artificial (1.94)\n",
      "  modelo (1.61)\n",
      "\n",
      "Topic 03\n",
      "  deepseek (8.98)\n",
      "  openai (6.84)\n",
      "  chatgpt (2.32)\n",
      "  mini (2.13)\n",
      "  o3 (1.97)\n",
      "\n",
      "Topic 04\n",
      "  deepseek (4.20)\n",
      "  openai (2.14)\n",
      "  intelligence (2.05)\n",
      "  artificial (1.90)\n",
      "  ki (1.72)\n"
     ]
    }
   ],
   "source": [
    "# Fit LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_output = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "print(\"\\nLDA Topics:\")\n",
    "display_topics(lda_model, count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154debfd-033f-49ce-9625-0a7856a4579c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1114821133131676161655431019\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1114821133131676161655431019_data = {\"mdsDat\": {\"x\": [0.022976594395828602, -0.06546352644591834, 0.2017779815313234, -0.11840856453034385, -0.04088248495089], \"y\": [-0.1631858798290428, -0.002423604228008914, 0.07804475383470288, 0.11063439666206132, -0.023069666439712377], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [28.373014170800076, 22.295087735393892, 21.365391506230356, 15.610558239574553, 12.355948348001126]}, \"tinfo\": {\"Term\": [\"openai\", \"nvidia\", \"mini\", \"artificial\", \"di\", \"ia\", \"o3\", \"deepseek\", \"inteligencia\", \"including\", \"intelligence\", \"altman\", \"ki\", \"modelo\", \"o1\", \"framework\", \"information\", \"device\", \"part\", \"store\", \"chatgpt\", \"iab\", \"words\", \"partners\", \"access\", \"sam\", \"app\", \"modelos\", \"week\", \"open\", \"mini\", \"o1\", \"altman\", \"o3\", \"sam\", \"api\", \"recently\", \"reddit\", \"app\", \"chaos\", \"stargate\", \"launched\", \"openai\", \"rivals\", \"plusteam\", \"kakao\", \"justin\", \"sullivan\", \"power\", \"iphone\", \"month\", \"drove\", \"kevin\", \"sparked\", \"sorgt\", \"reports\", \"unexpected\", \"cofounder\", \"benchmarks\", \"stem\", \"google\", \"said\", \"gpt\", \"stocks\", \"chatgpt\", \"january\", \"model\", \"us\", \"images\", \"illustration\", \"deepseek\", \"flyer\", \"chinese\", \"top\", \"th\", \"getty\", \"global\", \"tech\", \"r1\", \"ceo\", \"models\", \"china\", \"gpu\", \"startup\", \"nvidia\", \"new\", \"mercado\", \"si\", \"growth\", \"tsmc\", \"giants\", \"unidos\", \"help\", \"inside\", \"llega\", \"nvidia\", \"dng\", \"pa\\u00eds\", \"medio\", \"nie\", \"hype\", \"chinas\", \"passada\", \"central\", \"h800\", \"duda\", \"d\\u00eda\", \"come\", \"vn\", \"intelig\\u00eancia\", \"shockwaves\", \"quickly\", \"aten\\u00e7\\u00e3o\", \"mas\", \"centers\", \"fallout\", \"na\", \"uma\", \"deepseekdeepseek\", \"chatbot\", \"founder\", \"android\", \"deepseek\", \"artificial\", \"r1\", \"ceo\", \"d\\u00edas\", \"apple\", \"data\", \"ia\", \"hace\", \"chatgpt\", \"could\", \"v3\", \"modelo\", \"intelligence\", \"china\", \"stock\", \"inteligencia\", \"microsoft\", \"getty\", \"gpu\", \"images\", \"startup\", \"semana\", \"com\", \"donald\", \"including\", \"framework\", \"device\", \"information\", \"part\", \"di\", \"iab\", \"partners\", \"words\", \"newsletter\", \"artificiale\", \"che\", \"intelligenza\", \"modello\", \"business\", \"cinese\", \"della\", \"cuda\", \"back\", \"sulla\", \"insider\", \"suo\", \"originally\", \"deepseeks\", \"landscape\", \"stories\", \"mobile\", \"ptx\", \"success\", \"access\", \"store\", \"meta\", \"week\", \"per\", \"new\", \"sign\", \"first\", \"tech\", \"cost\", \"news\", \"chinese\", \"deepseek\", \"one\", \"world\", \"r1\", \"model\", \"startup\", \"models\", \"c\\u00f3digo\", \"janus\", \"ollama\", \"abierto\", \"vamos\", \"exclusive\", \"newsletters\", \"7b\", \"coverage\", \"learn\", \"updates\", \"atenci\\u00f3n\", \"nuevo\", \"explicarte\", \"competir\", \"embargo\", \"haber\", \"figure\", \"qwen\", \"im\\u00e1genes\", \"riesgo\", \"lanzar\", \"nombre\", \"max\", \"mod\\u00e8les\", \"cookies\", \"demostrado\", \"imposible\", \"chino\", \"weekly\", \"modelos\", \"inteligencia\", \"modelo\", \"source\", \"content\", \"join\", \"llama\", \"ia\", \"open\", \"pro\", \"deepseek\", \"artificial\", \"alphabet\", \"daily\", \"china\", \"r1\", \"startup\", \"latest\", \"chinese\", \"industry\", \"openai\", \"model\", \"company\", \"government\", \"modell\", \"chinesische\", \"ki\", \"devices\", \"productos\", \"deepseekai\", \"unternehmen\", \"inhaltsverzeichnis\", \"japan\", \"banned\", \"federal\", \"wanken\", \"cantidad\", \"hacen\", \"gro\\u00dfen\", \"az\", \"bringt\", \"weniger\", \"kaum\", \"dennoch\", \"k\\u00fcnstlichen\", \"ups\", \"legal\", \"veces\", \"aware\", \"portfolio\", \"ability\", \"deepseekchatgpt\", \"patron\", \"woche\", \"intelligence\", \"start\", \"february\", \"based\", \"microsoft\", \"artificial\", \"technology\", \"release\", \"deepseek\", \"market\", \"openai\", \"home\", \"apple\", \"chinese\", \"startup\", \"r1\", \"security\", \"ia\", \"hace\", \"intelligenz\", \"may\", \"china\", \"models\", \"model\", \"us\", \"chatgpt\", \"world\", \"latest\", \"app\", \"news\"], \"Freq\": [128.0, 50.0, 33.0, 47.0, 23.0, 37.0, 32.0, 411.0, 23.0, 19.0, 24.0, 25.0, 14.0, 21.0, 21.0, 16.0, 16.0, 16.0, 16.0, 19.0, 56.0, 14.0, 14.0, 14.0, 18.0, 21.0, 22.0, 15.0, 28.0, 22.0, 32.82438742220028, 20.47515115345657, 24.111812997809217, 30.364700972723256, 19.50691178090953, 11.002489024349021, 4.588897424013209, 4.587043853622398, 19.25324769423198, 3.7070785625038836, 3.706938243868088, 3.7031299073362662, 105.54958069298293, 4.241186374792842, 2.8244414079097893, 2.824440340426228, 2.8244387044705666, 2.8244387044705666, 2.8244378099994734, 2.8244359326905157, 2.8244358226085238, 2.8244345045526305, 2.8242469162994817, 2.8238391045250015, 2.822921992588469, 2.822837430801699, 2.8227360076815207, 2.819105927939287, 2.8190777245957728, 2.817738298724609, 14.419747325448048, 6.822894023255934, 5.452202049716369, 9.887332300905303, 35.80887744710913, 8.057493128949345, 26.13192381969325, 13.115240267555171, 14.073340561405473, 6.460253438285827, 138.62809823219226, 5.474778126469805, 25.72193847755705, 8.998532110347526, 7.2404296136580575, 13.8643231682911, 8.337954015141843, 15.747464997612928, 26.291415068731702, 10.335187941546826, 10.24760030290952, 14.59164805990461, 7.32255805699196, 9.077615512289222, 7.963766591944917, 6.92964734868802, 6.08943354467244, 6.086098459227258, 6.086300061584878, 5.2417647067204065, 4.392642254129467, 4.391869590599746, 3.5521698830690625, 3.5521563149810875, 3.506296377643374, 41.15188713388377, 2.7064164715591423, 2.7064090005018047, 2.706407505431169, 2.706206919604895, 2.705864566818381, 2.704478947223053, 2.704224809734427, 2.703116839673147, 2.7030372260507427, 2.698351422187889, 2.6977377047719635, 2.6907411236214642, 4.39836757548843, 4.397643848008329, 1.8606572593761355, 1.860656874685442, 1.8606559096022424, 1.8606559096022424, 1.8606558697259872, 1.8606558144512244, 6.049223576312332, 3.5567146488292836, 3.5493427956185117, 8.081192708106991, 4.400644241481892, 4.393962969233847, 135.73741423441882, 17.98248265357214, 24.979248998754176, 9.453595600126036, 5.028179149486245, 7.271249392825914, 6.940795310792049, 12.844388705690925, 4.392581162804547, 15.055876172706864, 4.399091264956765, 5.431333379055741, 7.344634406155636, 7.3176882455026595, 9.361628143610524, 4.638740934985069, 6.119909290567934, 5.217015574100003, 5.926843701609448, 4.982477595207865, 4.8786202315819045, 4.911038455529656, 4.439890052309295, 4.461806284953547, 4.4009728891307445, 18.815632627289972, 15.427283716087384, 15.426957648479894, 15.426970840015992, 15.426631149120231, 22.703417813156566, 13.732185020008503, 13.732185020008503, 13.732185020008503, 8.643677746282814, 7.797245343140261, 7.732557108977708, 6.949405376934269, 6.102458638079172, 6.102414352628503, 5.255454084705779, 5.254338386871462, 4.401464798173361, 4.366280621165863, 3.5601899675357207, 3.559033853139312, 3.5589989785149068, 3.558921985019705, 3.5587691871505136, 3.5581572613602073, 3.5571889241125296, 3.5565940386699944, 3.5563920214922247, 3.551610870442413, 15.374835039504081, 16.101118204781233, 16.19003322853049, 18.644883789401398, 5.254656025046394, 16.768146734627926, 5.195797574964094, 5.761220481510915, 16.00224378776415, 7.005837837997916, 8.726994315453036, 14.746851349227944, 40.80050594362171, 6.551940588163386, 7.151868404766529, 11.826220227133604, 7.6691647734867, 6.993713466176473, 6.255574247618037, 6.7258933510858485, 5.0857954964614045, 5.085539368739092, 5.0847822007510315, 5.084594578922814, 5.084318373235511, 5.084318373235417, 4.266037543838135, 4.265773495868004, 4.265773495868004, 4.265773495868004, 4.265448067038212, 4.264814451048269, 3.4446100773122277, 3.444635936614161, 3.44372279300646, 3.429476888477589, 5.910860636733155, 2.6252510403066176, 2.6252502241839624, 2.625247686160716, 2.6250944640391656, 2.624842047572928, 2.6244940197460735, 2.6241657256104967, 2.6241838995082207, 2.62415442353412, 2.6236756256684752, 5.056170363639291, 4.271247360917532, 11.40468527333132, 16.541464578619518, 13.712133093979485, 9.310268242953327, 4.26985819898346, 4.269092145362936, 4.266789025926462, 19.246591830366448, 11.581194322309479, 7.3533261063885655, 67.60956446671472, 16.50730062178588, 5.817673432499156, 5.0904015366590585, 10.020368355598459, 11.893926961310534, 7.676713792951797, 5.066930161435755, 5.633475825193152, 4.87003170624022, 5.596291066127962, 5.035858665960974, 4.652969622681653, 6.455224367145526, 3.3063923426157285, 3.286930718498485, 11.547800882261026, 2.519868866811743, 2.519865463409981, 2.51985816344368, 2.5191107432891613, 2.5188353494548164, 1.7324088366496446, 1.7324080927062318, 1.7324076276096356, 1.7324049879163108, 1.7324025401356598, 1.732401928687696, 1.732401822861837, 1.7323648831829042, 1.7320158770706606, 1.7320153841676842, 1.7319713130725227, 1.7319199651537998, 1.7319164727398637, 1.7319164727398633, 1.7318593777678541, 1.7315064161370055, 1.7314240292906253, 1.7311748970306016, 1.7310044493989656, 1.7310151281233512, 1.7309744795592787, 3.3166681366021855, 13.75644620349367, 5.913478054518391, 2.5197952098526106, 2.520030997812133, 6.163975836702662, 12.767696385627675, 3.2226928566890116, 4.466494070088059, 28.27220640758877, 4.068585434729047, 14.408294323942847, 2.520773833908842, 4.100071450830419, 6.60944541360888, 4.961621466299704, 6.479706287240487, 2.52212297125282, 4.685750951717974, 2.5215296579837427, 2.199251863040357, 2.5236631741050073, 4.203009698557109, 3.2917561132509157, 3.843884649719654, 3.098290942451232, 3.4761417766552443, 2.916518869113216, 2.8373569512703503, 2.8204806712250474, 2.7191125100291944], \"Total\": [128.0, 50.0, 33.0, 47.0, 23.0, 37.0, 32.0, 411.0, 23.0, 19.0, 24.0, 25.0, 14.0, 21.0, 21.0, 16.0, 16.0, 16.0, 16.0, 19.0, 56.0, 14.0, 14.0, 14.0, 18.0, 21.0, 22.0, 15.0, 28.0, 22.0, 33.49366267099726, 21.137322623382012, 25.46683802202109, 32.44087495048425, 21.03298669809397, 12.28538083001132, 5.2499496799827154, 5.249868623791669, 22.5806931856217, 4.367342368841701, 4.36732721505957, 4.367185693204928, 128.34887469699046, 5.226838299969956, 3.484702627399298, 3.4847025486749175, 3.4847024401829874, 3.4847024401829874, 3.4847023738683416, 3.484702244354277, 3.4847022348892343, 3.484702140878966, 3.4846817145817184, 3.484677321595818, 3.4845399288277443, 3.4845895040562613, 3.4845189087256037, 3.4843263492188177, 3.484409275834197, 3.4841339147211925, 19.027737519928184, 8.72886928891048, 6.9180023644645985, 13.08885605450442, 56.983065835316935, 11.28414777468278, 45.49411049167737, 20.61672708607651, 24.26374809655904, 9.50790472234168, 411.0477892845363, 7.824236202769606, 52.88152137253954, 14.610336946406314, 11.286772116939352, 25.88881283946336, 13.703854959683579, 37.76652514811135, 81.47051754317052, 20.68391498267988, 22.183192106205432, 42.28589044842548, 12.976396534633677, 33.620702693246855, 50.96306635730039, 29.764254210003692, 6.75707983808186, 6.75698366117899, 6.757216620677976, 5.911366312209222, 5.065545826506073, 5.065385013205123, 4.219809886703702, 4.219809560687448, 4.221112784397246, 50.96306635730039, 3.374053264335016, 3.374053077700233, 3.374053041898867, 3.374061533079295, 3.374076573125585, 3.373965784346319, 3.3740668465917145, 3.374060410933952, 3.374151446179612, 3.3738644133990596, 3.3737930624832324, 3.3741931642461145, 5.853057556386403, 5.853134596842431, 2.5282965008389593, 2.5282964953196014, 2.5282964596398476, 2.5282964596398476, 2.528296465194675, 2.5282964606326224, 8.362137226713429, 5.008025855475605, 5.040240542446261, 13.573158487758837, 6.757635091991146, 6.830819425571485, 411.0477892845363, 47.606163512188374, 81.47051754317052, 20.68391498267988, 8.300305415703779, 14.172580697198356, 13.537039597616653, 37.124926959529134, 7.427823654149146, 56.983065835316935, 7.52289728716998, 10.837113485563897, 21.561854018705446, 24.30735475093639, 42.28589044842548, 9.155639010339208, 23.165836489136037, 14.777631067908072, 25.88881283946336, 12.976396534633677, 24.26374809655904, 33.620702693246855, 9.334650883753017, 13.567364268672076, 11.053899019711618, 19.485427635693203, 16.094771051089943, 16.094776808623784, 16.094800516565996, 16.094759076819013, 23.723005187368233, 14.399445293880847, 14.399445293880847, 14.399445293880847, 9.313350256438367, 8.465779582575186, 8.465656124945584, 7.618113041714682, 6.770467293863492, 6.770479594540037, 5.922784161118019, 5.922817879533369, 5.075100521329942, 5.076830211308747, 4.2274486105383415, 4.227496266063051, 4.2274667453820065, 4.227424954608608, 4.227402898505458, 4.227532177616219, 4.227392897943032, 4.2275958985985405, 4.227440034136359, 4.227802394679507, 18.631925272641443, 19.597413377594957, 22.69023676704671, 28.013231643608222, 6.7432752857878935, 29.764254210003692, 6.8077544764804525, 8.389227423702891, 37.76652514811135, 11.947309598828161, 16.848897440270214, 52.88152137253954, 411.0477892845363, 11.16511151633677, 14.240819812860487, 81.47051754317052, 45.49411049167737, 33.620702693246855, 22.183192106205432, 7.399977109993079, 5.7591821074183045, 5.759206767518901, 5.759190544502639, 5.759074930231075, 5.7592177126946185, 5.75921771269462, 4.938746980061423, 4.938766995258341, 4.938766995258341, 4.938766995258341, 4.938746461821527, 4.938806021136001, 4.118326123857877, 4.118361605339429, 4.118413523080828, 4.119580595464924, 7.42521252356134, 3.2979615356543706, 3.2979615453254993, 3.2979615777527806, 3.2979733004461877, 3.2979741434024286, 3.29800129024207, 3.2979182803547005, 3.2979962691006373, 3.2980444117614924, 3.2980101556402537, 6.605781059263137, 5.784368744051046, 15.580060964461039, 23.165836489136037, 21.561854018705446, 14.160744622673988, 5.784397182532113, 5.784420908755219, 5.786413470203724, 37.124926959529134, 22.628160567705244, 12.553483623381823, 411.0477892845363, 47.606163512188374, 10.079108982024476, 9.149603924283996, 42.28589044842548, 81.47051754317052, 33.620702693246855, 14.029351696014515, 52.88152137253954, 16.123617167442863, 128.34887469699046, 45.49411049167737, 17.803043868268823, 7.136586271025501, 3.9867390491517023, 3.989044942708123, 14.464910821860022, 3.199165350996407, 3.199165591890397, 3.1991661354910264, 3.1992324133542547, 3.1992899982501646, 2.411704736060854, 2.4117047933853644, 2.411704827189369, 2.4117050230223134, 2.4117051912039322, 2.4117052398718846, 2.411705254890849, 2.411707964787741, 2.4117521297151168, 2.4117521664302703, 2.411757045030704, 2.4117409342077365, 2.411741191353232, 2.411741191353232, 2.4117710045223344, 2.411742545452623, 2.411780018466425, 2.4118536270894895, 2.4118306494698807, 2.4118728049224716, 2.4118776310912953, 4.83170848522283, 24.30735475093639, 9.721072460234172, 4.0196171132042675, 4.081733912374339, 14.777631067908072, 47.606163512188374, 6.530171892948906, 12.387439528857218, 411.0477892845363, 11.706947991201213, 128.34887469699046, 4.8399133831038235, 14.172580697198356, 52.88152137253954, 33.620702693246855, 81.47051754317052, 6.531328361748146, 37.124926959529134, 7.427823654149146, 4.914414179461272, 7.538203076750413, 42.28589044842548, 22.183192106205432, 45.49411049167737, 20.61672708607651, 56.983065835316935, 14.240819812860487, 14.029351696014515, 22.5806931856217, 16.848897440270214], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.851, -4.323, -4.1595, -3.9289, -4.3714, -4.944, -5.8185, -5.8189, -4.3845, -6.0319, -6.032, -6.033, -2.683, -5.8973, -6.3039, -6.3039, -6.3039, -6.3039, -6.3039, -6.3039, -6.3039, -6.3039, -6.3039, -6.3041, -6.3044, -6.3044, -6.3045, -6.3058, -6.3058, -6.3062, -4.6736, -5.4219, -5.6462, -5.0509, -3.764, -5.2556, -4.079, -4.7684, -4.6979, -5.4765, -2.4104, -5.642, -4.0948, -5.1451, -5.3625, -4.7129, -5.2214, -4.5855, -4.0729, -5.0066, -5.0151, -4.6617, -5.3512, -5.1364, -5.2673, -5.4064, -5.2945, -5.2951, -5.2951, -5.4444, -5.6212, -5.6213, -5.8335, -5.8335, -5.8465, -3.3838, -6.1055, -6.1055, -6.1055, -6.1056, -6.1057, -6.1062, -6.1063, -6.1067, -6.1067, -6.1085, -6.1087, -6.1113, -5.6199, -5.62, -6.4802, -6.4802, -6.4802, -6.4802, -6.4802, -6.4802, -5.3012, -5.8323, -5.8343, -5.0116, -5.6193, -5.6209, -2.1904, -4.2117, -3.8831, -4.8547, -5.486, -5.1172, -5.1637, -4.5482, -5.6212, -4.3893, -5.6197, -5.4089, -5.1071, -5.1108, -4.8645, -5.5667, -5.2896, -5.4492, -5.3216, -5.4952, -5.5162, -5.5096, -5.6105, -5.6055, -5.6193, -4.1238, -4.3224, -4.3224, -4.3224, -4.3224, -3.936, -4.4388, -4.4388, -4.4388, -4.9017, -5.0047, -5.0131, -5.1198, -5.2498, -5.2498, -5.3992, -5.3995, -5.5766, -5.5846, -5.7887, -5.789, -5.789, -5.789, -5.7891, -5.7893, -5.7895, -5.7897, -5.7898, -5.7911, -4.3258, -4.2796, -4.2741, -4.1329, -5.3994, -4.239, -5.4107, -5.3074, -4.2858, -5.1118, -4.8921, -4.3675, -3.3498, -5.1787, -5.0911, -4.5882, -5.0213, -5.1135, -5.225, -4.8387, -5.1182, -5.1183, -5.1184, -5.1185, -5.1185, -5.1185, -5.294, -5.2941, -5.2941, -5.2941, -5.2941, -5.2943, -5.5079, -5.5079, -5.5081, -5.5123, -4.9679, -5.7795, -5.7795, -5.7795, -5.7796, -5.7797, -5.7798, -5.7799, -5.7799, -5.7799, -5.7801, -5.1241, -5.2928, -4.3107, -3.9388, -4.1264, -4.5136, -5.2931, -5.2933, -5.2938, -3.7873, -4.2953, -4.7495, -2.5309, -3.9409, -4.9838, -5.1173, -4.4401, -4.2687, -4.7065, -5.1219, -5.016, -5.1616, -5.0226, -5.1281, -5.2072, -4.646, -5.315, -5.3209, -4.0644, -5.5867, -5.5867, -5.5867, -5.587, -5.5871, -5.9614, -5.9614, -5.9614, -5.9614, -5.9614, -5.9614, -5.9614, -5.9614, -5.9616, -5.9616, -5.9616, -5.9616, -5.9616, -5.9616, -5.9617, -5.9619, -5.9619, -5.9621, -5.9622, -5.9622, -5.9622, -5.3119, -3.8894, -4.7336, -5.5867, -5.5866, -4.6921, -3.964, -5.3407, -5.0143, -3.169, -5.1076, -3.8431, -5.5863, -5.0999, -4.6224, -4.9091, -4.6422, -5.5858, -4.9663, -5.586, -5.7228, -5.5852, -5.0751, -5.3194, -5.1644, -5.38, -5.2649, -5.4405, -5.468, -5.474, -5.5106], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2395, 1.2279, 1.2051, 1.1936, 1.1844, 1.1494, 1.1252, 1.1248, 1.1003, 1.0958, 1.0958, 1.0948, 1.0642, 1.0508, 1.0497, 1.0497, 1.0497, 1.0497, 1.0497, 1.0497, 1.0497, 1.0497, 1.0496, 1.0495, 1.0492, 1.0491, 1.0491, 1.0479, 1.0478, 1.0474, 0.9824, 1.0134, 1.0216, 0.9792, 0.7952, 0.9229, 0.7053, 0.8074, 0.715, 0.8733, 0.1728, 0.9027, 0.539, 0.7751, 0.8158, 0.6352, 0.7629, 0.385, 0.1287, 0.5659, 0.4874, 0.1957, 0.6876, -0.0496, -0.5965, -0.1978, 1.3968, 1.3962, 1.3962, 1.3806, 1.3583, 1.3581, 1.3286, 1.3286, 1.3153, 1.287, 1.2803, 1.2803, 1.2803, 1.2802, 1.2801, 1.2796, 1.2795, 1.2791, 1.279, 1.2774, 1.2772, 1.2745, 1.2151, 1.2149, 1.1942, 1.1942, 1.1942, 1.1942, 1.1942, 1.1942, 1.177, 1.1586, 1.1501, 0.9822, 1.0719, 1.0596, 0.3928, 0.5272, 0.3186, 0.7178, 0.9996, 0.8334, 0.8328, 0.4394, 0.9755, 0.1698, 0.9643, 0.81, 0.4238, 0.3003, -0.007, 0.8209, 0.1697, 0.4596, 0.0265, 0.5436, -0.1033, -0.4229, 0.7577, 0.3887, 0.5798, 1.5084, 1.501, 1.501, 1.501, 1.501, 1.4995, 1.496, 1.496, 1.496, 1.4688, 1.4611, 1.4528, 1.4515, 1.4395, 1.4395, 1.4239, 1.4236, 1.401, 1.3926, 1.3716, 1.3713, 1.3713, 1.3713, 1.3712, 1.371, 1.3708, 1.3706, 1.3705, 1.3691, 1.3513, 1.3469, 1.2059, 1.1363, 1.294, 0.9696, 1.2732, 1.1676, 0.6847, 1.0096, 0.8855, 0.2664, -0.7666, 1.0104, 0.8547, -0.3865, -0.237, -0.0267, 0.2775, 1.7617, 1.7329, 1.7328, 1.7327, 1.7327, 1.7326, 1.7326, 1.7108, 1.7107, 1.7107, 1.7107, 1.7107, 1.7105, 1.6786, 1.6786, 1.6783, 1.6739, 1.6291, 1.6291, 1.6291, 1.6291, 1.629, 1.6289, 1.6288, 1.6287, 1.6287, 1.6287, 1.6285, 1.5899, 1.554, 1.5453, 1.5204, 1.4046, 1.4379, 1.5536, 1.5535, 1.5526, 1.2003, 1.1874, 1.3224, 0.0523, 0.7981, 1.3077, 1.2709, 0.4174, -0.067, 0.3803, 0.8388, -0.3821, 0.66, -1.2754, -0.3438, 0.5154, 1.9907, 1.9039, 1.8974, 1.8658, 1.8523, 1.8523, 1.8523, 1.852, 1.8519, 1.7602, 1.7602, 1.7602, 1.7602, 1.7602, 1.7602, 1.7602, 1.7602, 1.76, 1.76, 1.7599, 1.7599, 1.7599, 1.7599, 1.7599, 1.7597, 1.7596, 1.7594, 1.7593, 1.7593, 1.7593, 1.7148, 1.5218, 1.594, 1.624, 1.6088, 1.2166, 0.775, 1.3848, 1.071, -0.5858, 1.0341, -0.0959, 1.4387, 0.8507, 0.0115, 0.1776, -0.4405, 1.1395, 0.0213, 1.0107, 1.287, 0.9968, -0.2176, 0.1831, -0.3801, 0.1958, -0.7058, 0.5053, 0.4928, 0.0108, 0.2671]}, \"token.table\": {\"Topic\": [4, 4, 5, 2, 3, 1, 2, 4, 1, 5, 1, 2, 1, 3, 1, 5, 1, 2, 5, 2, 4, 5, 3, 4, 2, 5, 5, 3, 5, 1, 5, 1, 5, 3, 5, 2, 2, 1, 2, 5, 1, 1, 2, 3, 5, 1, 2, 4, 5, 3, 1, 2, 3, 4, 5, 2, 1, 3, 4, 5, 5, 2, 4, 3, 1, 1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 4, 2, 4, 4, 1, 2, 3, 1, 2, 5, 4, 3, 4, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 5, 5, 2, 4, 3, 3, 4, 5, 3, 5, 2, 3, 2, 1, 2, 5, 1, 2, 2, 2, 4, 5, 4, 4, 4, 2, 4, 5, 5, 2, 4, 2, 3, 5, 1, 2, 1, 2, 5, 3, 1, 2, 3, 5, 2, 1, 2, 4, 5, 1, 3, 4, 5, 5, 1, 5, 1, 2, 2, 5, 2, 4, 2, 5, 5, 2, 4, 5, 2, 2, 4, 5, 3, 1, 3, 4, 1, 2, 3, 4, 4, 3, 1, 2, 3, 4, 5, 3, 5, 2, 3, 2, 4, 2, 5, 2, 3, 4, 5, 2, 5, 3, 1, 1, 2, 3, 4, 4, 5, 2, 4, 1, 1, 5, 1, 1, 5, 5, 3, 4, 2, 3, 4, 5, 1, 4, 5, 3, 4, 2, 1, 3, 4, 5, 2, 4, 1, 2, 5, 2, 2, 2, 3, 4, 5, 2, 3, 5, 1, 3, 1, 2, 3, 4, 5, 5, 3, 2, 4, 2, 4, 5, 1, 2, 3, 5, 4, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 3, 4, 2, 4, 4, 1, 2, 5, 1, 1, 5, 4, 1, 3, 1, 3, 4, 5, 1, 3, 4, 5, 3, 3, 3, 2, 5, 2, 3, 4, 1, 5, 1, 1, 4, 5, 5, 3, 2, 4, 1, 2, 3, 4, 5, 1, 1, 1, 2, 3, 4, 5, 1, 4, 1, 1, 3, 1, 5, 2, 4, 5, 1, 2, 4, 2, 2, 1, 3, 1, 1, 4, 5, 1, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 3, 3, 3, 3, 1, 3, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 1, 3, 5, 2, 2, 5, 1, 2, 5, 4, 5, 1, 2, 3, 5, 1, 2, 4, 5, 4, 5, 2, 5, 5, 1, 2, 3, 4, 2, 4, 5, 2, 5, 3, 1, 2, 3, 4, 5], \"Freq\": [0.8099220341006925, 0.8681775609547571, 0.829245619065169, 0.1610139562123035, 0.8050697810615175, 0.2976453578734322, 0.09921511929114407, 0.5952907157468644, 0.9424020359043899, 0.039266751496016244, 0.2927906412681484, 0.5855812825362968, 0.8953731391971724, 0.08139755810883387, 0.8414267818889761, 0.13285686029825938, 0.14111755951372787, 0.4939114582980476, 0.28223511902745574, 0.3781023017196407, 0.35709661829077183, 0.2730738845752961, 0.9449808989199431, 0.8099221190886371, 0.7910464741484063, 0.8292630275922664, 0.829287803167339, 0.7878931997942171, 0.8292888936844359, 0.24499392206051507, 0.7349817661815452, 0.86097807763463, 0.8292726169319257, 0.886200145236184, 0.829288756890552, 0.7910464724104272, 0.8891364215881331, 0.4834674677580968, 0.4351207209822871, 0.048346746775809676, 0.9158888088411702, 0.1473496387597427, 0.5893985550389708, 0.22102445813961402, 0.07367481937987135, 0.6317666393037059, 0.26323609970987744, 0.03509814662798366, 0.05264721994197549, 0.9449946799075096, 0.35472825192826296, 0.21283695115695778, 0.09459420051420345, 0.23648550128550863, 0.09459420051420345, 0.8891613583986678, 0.49166512848288346, 0.2836529587401251, 0.11346118349605003, 0.1323713807453917, 0.7520597143143065, 0.15138255280164375, 0.7569127640082187, 0.8441975705993262, 0.8609985688259646, 0.29482513484481726, 0.29482513484481726, 0.14741256742240863, 0.22111885113361293, 0.8891014396534351, 0.28085084983201825, 0.22468067986561457, 0.22468067986561457, 0.28085084983201825, 0.056170169966403644, 0.728445019521967, 0.1728788616071919, 0.6915154464287676, 0.9096432364425019, 0.25110255787581265, 0.16740170525054174, 0.5859059683768961, 0.13292750941920511, 0.5317100376768205, 0.26585501883841023, 0.8099187517532936, 0.7881617286571087, 0.9459488719967873, 0.4371773940272525, 0.5464717425340656, 0.22161418516705703, 0.5170997653897997, 0.07387139505568567, 0.14774279011137134, 0.3381601935919455, 0.330861772147515, 0.0997450930738832, 0.1654308860737575, 0.0681186001480178, 0.9377443599188208, 0.8292311252559146, 0.7936129171443503, 0.19840322928608758, 0.9462074223902687, 0.8441927646091875, 0.9096299580749714, 0.8292764664862338, 0.9319793730822543, 0.9377445898711128, 0.04215317545571626, 0.9695230354814739, 0.8891383048723929, 0.45232908235219643, 0.36186326588175716, 0.18093163294087858, 0.8609057183990174, 0.8891880740926387, 0.8892068791533683, 0.6023874724585725, 0.12047749449171449, 0.24095498898342899, 0.7284358365635452, 0.8681734654654345, 0.7284512954476088, 0.7910464738377897, 0.24877991406570626, 0.7463397421971188, 0.8292888820605899, 0.1346762798811275, 0.808057679286765, 0.11920048766047321, 0.7152029259628393, 0.11920048766047321, 0.6390400123950899, 0.25561600495803594, 0.14798076344565517, 0.5919230537826207, 0.14798076344565517, 0.9319797064764208, 0.5407741207298327, 0.23176033745564256, 0.1931336145463688, 0.03862672290927376, 0.7896483690009324, 0.58377734028387, 0.1459443350709675, 0.07297216753548375, 0.1459443350709675, 0.7357679800521465, 0.05255485571801047, 0.10510971143602094, 0.10510971143602094, 0.8407381025238867, 0.7227519935065767, 0.14455039870131534, 0.5394409751056216, 0.3853149822183011, 0.8879395669570821, 0.8292887349911745, 0.889112432518924, 0.7282294715395485, 0.5385157465020888, 0.40388680987656655, 0.8292887401555941, 0.9479100024396108, 0.4132305356914064, 0.6198458035371096, 0.8891321625285291, 0.35016903909795283, 0.5117855186816234, 0.13468039965305878, 0.9722596748882685, 0.6310538625719709, 0.10517564376199516, 0.21035128752399032, 0.576992472238261, 0.2060687400850932, 0.2060687400850932, 0.9096394063157759, 0.9096528139487171, 0.9750876580812626, 0.4341457581946651, 0.18606246779771363, 0.06202082259923787, 0.31010411299618934, 0.06202082259923787, 0.9319780002592052, 0.9377080544873503, 0.9479100756737375, 0.9461865246601596, 0.2590020870955292, 0.733839246770666, 0.6833945014963204, 0.1708486253740801, 0.28797868265490056, 0.04113981180784294, 0.08227962361568587, 0.5759573653098011, 0.40696610561612123, 0.40696610561612123, 0.9188627107093232, 0.8609056928351441, 0.7089591664112088, 0.0886198958014011, 0.0886198958014011, 0.0886198958014011, 0.8681788328171781, 0.8292889133960446, 0.17287815250207914, 0.6915126100083165, 0.8609056444551016, 0.8609056176518627, 0.8292709268211292, 0.8609107648042694, 0.13826562946917795, 0.8295937768150677, 0.8292763780668342, 0.9461784871039072, 0.9096495716305907, 0.0712791311863742, 0.356395655931871, 0.356395655931871, 0.2138373935591226, 0.9159216669498972, 0.8099187517532936, 0.8292661269456268, 0.1728186216123945, 0.691274486449578, 0.947617418512351, 0.4270967978808767, 0.08541935957617534, 0.08541935957617534, 0.3416774383047014, 0.7910464741484063, 0.9096418515287491, 0.39797282846527493, 0.26531521897684995, 0.39797282846527493, 0.8891383634892842, 0.887957541390132, 0.1322154559602011, 0.7051490984544059, 0.08814363730680073, 0.08814363730680073, 0.3383492237032686, 0.20300953422196116, 0.4060190684439223, 0.9852610126325559, 0.9461642257071001, 0.5715025465715261, 0.06594260152748378, 0.1758469374066234, 0.10990433587913964, 0.0879234687033117, 0.7524946988036099, 0.886201755296593, 0.3246474071259051, 0.6492948142518102, 0.12836920244164052, 0.7060306134290228, 0.12836920244164052, 0.4507917504443665, 0.0901583500888733, 0.2704750502666199, 0.13523752513330994, 0.9096647475683787, 0.86090569517351, 0.7175199159411763, 0.1195866526568627, 0.1195866526568627, 0.23518143443511236, 0.06719469555288925, 0.5711549121995586, 0.10079204332933386, 0.033597347776444624, 0.178053193725885, 0.11870212915058999, 0.5341595811776549, 0.178053193725885, 0.9663547222202079, 0.8681734654654343, 0.8891361258791529, 0.9096493391258014, 0.8099123518683852, 0.15697642571018514, 0.8045041817646988, 0.019622053213773142, 0.9461936289828916, 0.9247592750130861, 0.06165061833420574, 0.8681751153994474, 0.35825884892839693, 0.6269529856246947, 0.2651563295234505, 0.17677088634896698, 0.530312659046901, 0.044192721587241746, 0.825874011363541, 0.02337379277443984, 0.04674758554887968, 0.10907769961405259, 0.946202485661945, 0.9319803998560143, 0.9722596748882685, 0.8891347256591626, 0.8292294659638539, 0.8891383540548246, 0.7414794425696938, 0.14829588851393877, 0.8609055982027823, 0.8292377188799409, 0.8609056608383238, 0.31863665258221185, 0.5576141420188707, 0.07965916314555296, 0.937744519259877, 0.9461991105019131, 0.7910464629850228, 0.9096528166162344, 0.3191338509200316, 0.30685947203849195, 0.14729254657847612, 0.14729254657847612, 0.07364627328923806, 0.952390080816253, 0.9524047853960955, 0.16145386585669222, 0.08072693292834611, 0.32290773171338444, 0.16145386585669222, 0.32290773171338444, 0.860933546550556, 0.9096528050045354, 0.7652809921483494, 0.8019366275644764, 0.11456237536635376, 0.9508873032193957, 0.04754436516096979, 0.30621642171802993, 0.30621642171802993, 0.4593246325770449, 0.321383203009928, 0.4285109373465707, 0.21425546867328535, 0.7910464612581413, 0.8879701803146128, 0.1468913139354272, 0.734456569677136, 0.8609457952198725, 0.21185326619028505, 0.6355597985708551, 0.07061775539676168, 0.860911850118203, 0.9158919867984842, 0.30860792492516126, 0.6172158498503225, 0.267692203881502, 0.14871789104527888, 0.20820504746339044, 0.2379486256724462, 0.14871789104527888, 0.8610461232056479, 0.5461115269347819, 0.21844461077391275, 0.10922230538695638, 0.21844461077391275, 0.7640087077402447, 0.2292026123220734, 0.15308142672694733, 0.8164342758770524, 0.9462096607926657, 0.9461180127609119, 0.946197190908164, 0.8609056444551016, 0.9461931319435011, 0.4236556034014725, 0.0794354256377761, 0.4236556034014725, 0.052956950425184064, 0.026478475212592032, 0.45940597723611454, 0.45940597723611454, 0.6201950325101628, 0.08859929035859468, 0.26579787107578406, 0.6160022204151643, 0.20533407347172142, 0.13688938231448095, 0.8458281446157543, 0.7987179210799273, 0.19967948026998183, 0.8609509888115926, 0.7896734383610061, 0.9377249328549506, 0.8099187517532936, 0.8292763780668342, 0.6305559532181779, 0.09700860818741198, 0.09700860818741198, 0.14551291228111798, 0.09227549396175454, 0.46137746980877264, 0.18455098792350907, 0.18455098792350907, 0.8681949897462754, 0.8292759124604864, 0.6834034966281016, 0.1708508741570254, 0.8292888147214742, 0.10709224976849502, 0.14278966635799337, 0.6782509152004684, 0.07139483317899668, 0.17287971155512058, 0.6915188462204823, 0.8292726043075475, 0.20696612865994993, 0.6208983859798498, 0.9722596748882685, 0.14044135283516865, 0.07022067641758432, 0.49154473492309025, 0.07022067641758432, 0.21066202925275299], \"Term\": [\"7b\", \"abierto\", \"ability\", \"access\", \"access\", \"alphabet\", \"alphabet\", \"alphabet\", \"altman\", \"altman\", \"android\", \"android\", \"api\", \"api\", \"app\", \"app\", \"apple\", \"apple\", \"apple\", \"artificial\", \"artificial\", \"artificial\", \"artificiale\", \"atenci\\u00f3n\", \"aten\\u00e7\\u00e3o\", \"aware\", \"az\", \"back\", \"banned\", \"based\", \"based\", \"benchmarks\", \"bringt\", \"business\", \"cantidad\", \"centers\", \"central\", \"ceo\", \"ceo\", \"ceo\", \"chaos\", \"chatbot\", \"chatbot\", \"chatbot\", \"chatbot\", \"chatgpt\", \"chatgpt\", \"chatgpt\", \"chatgpt\", \"che\", \"china\", \"china\", \"china\", \"china\", \"china\", \"chinas\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinesische\", \"chino\", \"chino\", \"cinese\", \"cofounder\", \"com\", \"com\", \"com\", \"com\", \"come\", \"company\", \"company\", \"company\", \"company\", \"company\", \"competir\", \"content\", \"content\", \"cookies\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"coverage\", \"cuda\", \"c\\u00f3digo\", \"daily\", \"daily\", \"data\", \"data\", \"data\", \"data\", \"deepseek\", \"deepseek\", \"deepseek\", \"deepseek\", \"deepseek\", \"deepseekai\", \"deepseekchatgpt\", \"deepseekdeepseek\", \"deepseekdeepseek\", \"deepseeks\", \"della\", \"demostrado\", \"dennoch\", \"device\", \"devices\", \"di\", \"di\", \"dng\", \"donald\", \"donald\", \"donald\", \"drove\", \"duda\", \"d\\u00eda\", \"d\\u00edas\", \"d\\u00edas\", \"d\\u00edas\", \"embargo\", \"exclusive\", \"explicarte\", \"fallout\", \"february\", \"february\", \"federal\", \"figure\", \"figure\", \"first\", \"first\", \"first\", \"flyer\", \"flyer\", \"founder\", \"founder\", \"founder\", \"framework\", \"getty\", \"getty\", \"getty\", \"getty\", \"giants\", \"global\", \"global\", \"global\", \"global\", \"google\", \"google\", \"google\", \"google\", \"government\", \"gpt\", \"gpt\", \"gpu\", \"gpu\", \"growth\", \"gro\\u00dfen\", \"h800\", \"haber\", \"hace\", \"hace\", \"hacen\", \"help\", \"home\", \"home\", \"hype\", \"ia\", \"ia\", \"ia\", \"iab\", \"illustration\", \"illustration\", \"illustration\", \"images\", \"images\", \"images\", \"imposible\", \"im\\u00e1genes\", \"including\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"information\", \"inhaltsverzeichnis\", \"inside\", \"insider\", \"inteligencia\", \"inteligencia\", \"intelig\\u00eancia\", \"intelig\\u00eancia\", \"intelligence\", \"intelligence\", \"intelligence\", \"intelligence\", \"intelligenz\", \"intelligenz\", \"intelligenza\", \"iphone\", \"january\", \"january\", \"january\", \"january\", \"janus\", \"japan\", \"join\", \"join\", \"justin\", \"kakao\", \"kaum\", \"kevin\", \"ki\", \"ki\", \"k\\u00fcnstlichen\", \"landscape\", \"lanzar\", \"latest\", \"latest\", \"latest\", \"latest\", \"launched\", \"learn\", \"legal\", \"llama\", \"llama\", \"llega\", \"market\", \"market\", \"market\", \"market\", \"mas\", \"max\", \"may\", \"may\", \"may\", \"medio\", \"mercado\", \"meta\", \"meta\", \"meta\", \"meta\", \"microsoft\", \"microsoft\", \"microsoft\", \"mini\", \"mobile\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modell\", \"modello\", \"modelo\", \"modelo\", \"modelos\", \"modelos\", \"modelos\", \"models\", \"models\", \"models\", \"models\", \"mod\\u00e8les\", \"month\", \"na\", \"na\", \"na\", \"new\", \"new\", \"new\", \"new\", \"new\", \"news\", \"news\", \"news\", \"news\", \"newsletter\", \"newsletters\", \"nie\", \"nombre\", \"nuevo\", \"nvidia\", \"nvidia\", \"nvidia\", \"o1\", \"o3\", \"o3\", \"ollama\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"openai\", \"openai\", \"openai\", \"openai\", \"originally\", \"part\", \"partners\", \"passada\", \"patron\", \"pa\\u00eds\", \"per\", \"per\", \"plusteam\", \"portfolio\", \"power\", \"pro\", \"pro\", \"pro\", \"productos\", \"ptx\", \"quickly\", \"qwen\", \"r1\", \"r1\", \"r1\", \"r1\", \"r1\", \"recently\", \"reddit\", \"release\", \"release\", \"release\", \"release\", \"release\", \"reports\", \"riesgo\", \"rivals\", \"said\", \"said\", \"sam\", \"sam\", \"security\", \"security\", \"security\", \"semana\", \"semana\", \"semana\", \"shockwaves\", \"si\", \"sign\", \"sign\", \"sorgt\", \"source\", \"source\", \"source\", \"sparked\", \"stargate\", \"start\", \"start\", \"startup\", \"startup\", \"startup\", \"startup\", \"startup\", \"stem\", \"stock\", \"stock\", \"stock\", \"stock\", \"stocks\", \"stocks\", \"store\", \"store\", \"stories\", \"success\", \"sulla\", \"sullivan\", \"suo\", \"tech\", \"tech\", \"tech\", \"tech\", \"tech\", \"technology\", \"technology\", \"th\", \"th\", \"th\", \"top\", \"top\", \"top\", \"tsmc\", \"uma\", \"uma\", \"unexpected\", \"unidos\", \"unternehmen\", \"updates\", \"ups\", \"us\", \"us\", \"us\", \"us\", \"v3\", \"v3\", \"v3\", \"v3\", \"vamos\", \"veces\", \"vn\", \"vn\", \"wanken\", \"week\", \"week\", \"week\", \"week\", \"weekly\", \"weekly\", \"weniger\", \"woche\", \"woche\", \"words\", \"world\", \"world\", \"world\", \"world\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 2, 1, 3, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1114821133131676161655431019\", ldavis_el1114821133131676161655431019_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1114821133131676161655431019\", ldavis_el1114821133131676161655431019_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1114821133131676161655431019\", ldavis_el1114821133131676161655431019_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize LDA results\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis = pyLDAvis.lda_model.prepare(lda_model, count_matrix, count_vectorizer)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5414e5b-3a1a-4756-bd0d-80be8cd47617",
   "metadata": {},
   "source": [
    "The LDA visualization show five distinct topics, with Topic 1 being the most prevalent and Topics 2 and 5 showing some overlap. The most salient terms include AI-related words like \"openai,\" \"nvidia,\" and \"deepseek,\" alongside technical and business-related terms, indicating a focus on AI companies, technologies, and partnerships. Term frequency analysis highlights \"deepseek\" as the most frequent term. The marginal topic distribution shows the relative importance of different topics within the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460aba72-5854-4391-a614-fd1544457ada",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "The higher coherence score will be selected as the best model as it indicates more interpretable topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cac8ea-5112-48c1-92a0-e21c432dd43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Scores:\n",
      "NMF: 0.3922\n",
      "LSA: 0.3927\n",
      "LDA: 0.2323\n"
     ]
    }
   ],
   "source": [
    "# Calculate coherence scores for each model\n",
    "def calculate_coherence(model, feature_names, doc_term_matrix):\n",
    "    coherence_scores = []\n",
    "    for topic in model.components_:\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "        word_indices = [list(feature_names).index(word) for word in top_words]\n",
    "        topic_vectors = doc_term_matrix[:, word_indices]\n",
    "        pairwise_similarities = cosine_similarity(topic_vectors.T)\n",
    "        coherence = pairwise_similarities.mean()\n",
    "        coherence_scores.append(coherence)\n",
    "    return coherence_scores\n",
    "\n",
    "nmf_coherence = calculate_coherence(nmf_model, tfidf_vectorizer.get_feature_names_out(), tfidf_matrix)\n",
    "lsa_coherence = calculate_coherence(lsa_model, tfidf_vectorizer.get_feature_names_out(), tfidf_matrix)\n",
    "lda_coherence = calculate_coherence(lda_model, count_vectorizer.get_feature_names_out(), count_matrix)\n",
    "\n",
    "print(\"\\nCoherence Scores:\")\n",
    "print(f\"NMF: {np.mean(nmf_coherence):.4f}\")\n",
    "print(f\"LSA: {np.mean(lsa_coherence):.4f}\")\n",
    "print(f\"LDA: {np.mean(lda_coherence):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b29aa-e43b-4f09-aa0a-9759dbd0b521",
   "metadata": {},
   "source": [
    "Use the most frequent words to try to discern what semantic groups the unsupervised topics might have identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "947946ed-d656-4a28-910f-a57fb1d5672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified semantic groups:\n",
      "\n",
      "Group_0-1-2-3-4: Topic 1, Topic 2, Topic 3, Topic 4, Topic 5\n",
      "Words: deepseek\n",
      "\n",
      "Group_0-3: Topic 1, Topic 4\n",
      "Words: tech\n",
      "\n",
      "Group_0-3-4: Topic 1, Topic 4, Topic 5\n",
      "Words: chinese\n",
      "\n",
      "Group_1-2-3-4: Topic 2, Topic 3, Topic 4, Topic 5\n",
      "Words: r1, china\n",
      "\n",
      "Group_1-2-4: Topic 2, Topic 3, Topic 5\n",
      "Words: artificial, ia\n",
      "\n",
      "Group_1-3: Topic 2, Topic 4\n",
      "Words: chatgpt\n",
      "\n",
      "Group_1-2: Topic 2, Topic 3\n",
      "Words: modelo, inteligencia\n",
      "\n",
      "Group_1-4: Topic 2, Topic 5\n",
      "Words: intelligence, apple\n",
      "\n",
      "Group_2-4: Topic 3, Topic 5\n",
      "Words: startup\n",
      "\n",
      "Group_3-4: Topic 4, Topic 5\n",
      "Words: openai\n"
     ]
    }
   ],
   "source": [
    "# Extract top words and their frequencies for each topic\n",
    "n_top_words = 15\n",
    "topic_word_freq = []\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = tfidf_vectorizer.get_feature_names_out()[top_features_ind]\n",
    "    weights = topic[top_features_ind]\n",
    "    topic_word_freq.append(dict(zip(top_features, weights)))\n",
    "\n",
    "# Calculate the relevance of each word across all topics\n",
    "word_topic_relevance = {}\n",
    "for topic_idx, word_freq in enumerate(topic_word_freq):\n",
    "    for word, freq in word_freq.items():\n",
    "        if word not in word_topic_relevance:\n",
    "            word_topic_relevance[word] = []\n",
    "        word_topic_relevance[word].append((topic_idx, freq))\n",
    "        \n",
    "# Identify semantic groups based on word relevance across topics\n",
    "threshold = 0.05\n",
    "semantic_groups = {}\n",
    "for word, relevances in word_topic_relevance.items():\n",
    "    relevant_topics = [topic for topic, score in relevances if score >= threshold]\n",
    "    if len(relevant_topics) > 1:\n",
    "        group_name = f\"Group_{'-'.join(map(str, relevant_topics))}\"\n",
    "        if group_name not in semantic_groups:\n",
    "            semantic_groups[group_name] = []\n",
    "        semantic_groups[group_name].append(word)\n",
    "\n",
    "# Display the identified semantic groups with interpretations\n",
    "topic_interpretations = [\n",
    "    \"Topic 1\",\n",
    "    \"Topic 2\",\n",
    "    \"Topic 3\",\n",
    "    \"Topic 4\",\n",
    "    \"Topic 5\"\n",
    "]\n",
    "\n",
    "print(\"\\nIdentified semantic groups:\")\n",
    "for group, words in semantic_groups.items():\n",
    "    topics = [int(t) for t in group.split('_')[1].split('-')]\n",
    "    interpretations = [topic_interpretations[t] for t in topics]\n",
    "    print(f\"\\n{group}: {', '.join(interpretations)}\")\n",
    "    print(f\"Words: {', '.join(words)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
