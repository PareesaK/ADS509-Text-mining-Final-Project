{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc42b3e7-0007-4be1-9f16-3d1397e2f5a8",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eed64f5a-12a8-4725-a9d6-1c9ea5cc39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Text normalization\n",
    "import textacy\n",
    "import textacy.preprocessing as tprep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9beca-402d-4f6d-a3cd-6c3d0e53d17c",
   "metadata": {},
   "source": [
    "## Data Loading and Column Standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04b589e-bdf9-4976-a0d0-5e6bca570922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 755 rows from 5 files\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 755 entries, 0 to 754\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   source       570 non-null    object             \n",
      " 1   date         570 non-null    datetime64[ns, UTC]\n",
      " 2   text         755 non-null    object             \n",
      " 3   title        755 non-null    object             \n",
      " 4   description  755 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), object(4)\n",
      "memory usage: 29.6+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "             source                      date  \\\n",
      "0       Gizmodo.com 2025-02-04 15:00:56+00:00   \n",
      "1  Business Insider 2025-02-04 18:25:21+00:00   \n",
      "2      Substack.com 2025-02-04 21:28:04+00:00   \n",
      "3  Business Insider 2025-02-04 21:26:32+00:00   \n",
      "4      heise online 2025-02-04 14:00:00+00:00   \n",
      "\n",
      "                                                text  \\\n",
      "0  Usually when large language models are given t...   \n",
      "1  Meta's CTO, Andrew Bosworth, talked about Deep...   \n",
      "2  The release of DeepSeek has upset American tec...   \n",
      "3  Sundar Pichai speaks during a Google I/O confe...   \n",
      "4  Inhaltsverzeichnis\\r\\nDer japanische Technolog...   \n",
      "\n",
      "                                               title  \\\n",
      "0    DeepSeek Gets an ‘F’ in Safety From Researchers   \n",
      "1  Meta's CTO says he called the whole DeepSeek t...   \n",
      "2                      DeepSeek and Double Standards   \n",
      "3  Alphabet is planning to spend big again this y...   \n",
      "4  KI-Update kompakt: Softbank, DeepSeek, OpenAI,...   \n",
      "\n",
      "                                         description  \n",
      "0  The model failed to block a single attack atte...  \n",
      "1  Meta's CTO said the DeepSeek news cycle has be...  \n",
      "2  The release of DeepSeek has upset American tec...  \n",
      "3  Alphabet's fourth-quarter earnings showed slow...  \n",
      "4  Das \"KI-Update\" liefert werktäglich eine Zusam...  \n"
     ]
    }
   ],
   "source": [
    "def load_news_data(directory='.'):\n",
    "    \"\"\"\n",
    "    Load and combine multiple CSV files containing news data from a directory.\n",
    "    \"\"\"\n",
    "    # Define column mapping\n",
    "    column_mapping = {\n",
    "        'source_name': 'source',\n",
    "        'publishedAt': 'date',\n",
    "        'content': 'text',\n",
    "        'title': 'title',\n",
    "        'description': 'description'\n",
    "    }\n",
    "    \n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    \n",
    "    # Initialize an empty list to store individual dataframes\n",
    "    dfs = []\n",
    "    \n",
    "    # Read each CSV file and append to the list\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(file)\n",
    "            dfs.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {str(e)}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No CSV files were successfully loaded\")\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Select and rename columns according to mapping\n",
    "    final_df = combined_df[column_mapping.keys()].rename(columns=column_mapping)\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "    \n",
    "    # Basic cleaning\n",
    "    for text_col in ['text', 'title', 'description']:\n",
    "        # Replace NaN with empty string\n",
    "        final_df[text_col] = final_df[text_col].fillna('')\n",
    "        # Basic string cleaning\n",
    "        final_df[text_col] = final_df[text_col].str.strip()\n",
    "    \n",
    "    print(f\"Loaded {len(final_df)} rows from {len(csv_files)} files\")\n",
    "    return final_df\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    news_df = load_news_data()\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(news_df.info())\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(news_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df76a53-082a-49cc-b9bb-c997796541ca",
   "metadata": {},
   "source": [
    "## Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d780061-568c-4df8-8496-9632a7a9115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 755/755 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_news_text(text):\n",
    "    # Handle None or empty strings\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "        \n",
    "    # convert html escapes\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove \"[+XXX chars]\" pattern found in your data\n",
    "    text = re.sub(r'\\[\\+\\d+ chars\\]', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # Clean special characters and encoding issues\n",
    "    text = re.sub(r'â€™', \"'\", text)  # Fix specific encoding issues in your data\n",
    "    text = re.sub(r'â€\"', \"-\", text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "news_df['clean_text'] = news_df['text'].progress_apply(clean_news_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e187d-57ed-4bf7-b91c-e97adddf3fbd",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b9cfed-c347-4113-bc1c-3b45e566620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 755/755 [00:00<00:00, 24388.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text\n",
    "\n",
    "news_df['normalized_text'] = news_df['clean_text'].progress_apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841309fe-b66d-4f3e-99eb-fd2588d0d937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
